{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80364b89",
   "metadata": {},
   "source": [
    "<div style=\"background:#2C3E50;padding:25px;color:#ffffff;margin-top:10px;border-radius:10px;\">\n",
    "\n",
    "#  Proyecto de Limpieza y An√°lisis de Datos de Texto\n",
    "\n",
    "## NLP - Portafolio de Ejercicios 1\n",
    "### Profesora: Lisibonny Beato\n",
    "### Per√≠odo 3-2024-2025\n",
    "\n",
    "---\n",
    "\n",
    "###  Estructura del Proyecto\n",
    "```\n",
    "proyecto_limpieza_datos_nlp/\n",
    " üìÅ datos/\n",
    "    originales/          # Datos sin procesar del corpus\n",
    "    procesados/          # Datos limpios y transformados\n",
    " üìÅ notebooks/           # An√°lisis exploratorio (este archivo)\n",
    " üìÅ src/                 # C√≥digo fuente modular\n",
    "    ingestion_datos.py  # Funciones de carga de datos\n",
    "    limpieza_datos.py   # Funciones de limpieza\n",
    "    utilidades.py       # Funciones de apoyo\n",
    " üìÅ tests/               # Pruebas unitarias\n",
    "  requirements.txt     # Dependencias del proyecto\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e48ad",
   "metadata": {},
   "source": [
    "##  Tabla de Contenidos\n",
    "\n",
    "1. [ Configuraci√≥n del Entorno](#configuracion)\n",
    "2. [ Recolecci√≥n de Datos](#recoleccion)\n",
    "3. [üîç Exploraci√≥n y Descripci√≥n de Datos](#exploracion)\n",
    "4. [üßπ Limpieza Avanzada de Datos](#limpieza)\n",
    "5. [ An√°lisis de Sentimientos](#sentimientos)\n",
    "6. [ An√°lisis Sem√°ntico con WordNet](#wordnet)\n",
    "7. [ Visualizaci√≥n de Resultados](#visualizacion)\n",
    "8. [ Conclusiones](#conclusiones)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e40f8",
   "metadata": {},
   "source": [
    "<a id=\"configuracion\"></a>\n",
    "<div style=\"background:#1ABC9C;padding:20px;color:#ffffff;margin-top:10px;border-radius:8px;\">\n",
    "\n",
    "#  1. Configuraci√≥n del Entorno\n",
    "\n",
    "Esta secci√≥n se encarga de verificar e instalar todas las dependencias necesarias para el proyecto de an√°lisis de texto y NLP.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bfeac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verificando dependencias...\n",
      "--------------------------------------------------\n",
      " numpy: Instalado\n",
      " pandas: Instalado\n",
      " matplotlib: Instalado\n",
      " seaborn: Instalado\n",
      " nltk: Instalado\n",
      " wordcloud: Instalado\n",
      " textblob: Instalado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cristiandelahoz/ghq/github.com/cristiandelahooz/first-porfolio/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spacy: Instalado\n",
      " scikit-learn: No encontrado\n",
      "\n",
      " Instalando paquetes faltantes: scikit-learn\n",
      " Instalaci√≥n completada exitosamente\n",
      " scikit-learn: Error en la instalaci√≥n\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "from typing import List, Dict\n",
    "\n",
    "def verificar_e_instalar_dependencias() -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Verifica e instala las dependencias necesarias para el proyecto de NLP.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, bool]: Diccionario con el estado de instalaci√≥n de cada paquete\n",
    "    \"\"\"\n",
    "    paquetes_requeridos = [\n",
    "        'numpy', 'pandas', 'matplotlib', 'seaborn', 'nltk', \n",
    "        'wordcloud', 'textblob', 'spacy', 'scikit-learn'\n",
    "    ]\n",
    "    \n",
    "    estado_instalacion: Dict[str, Any] = {}\n",
    "    paquetes_faltantes: List[Any] = []\n",
    "    \n",
    "    print(\"üîç Verificando dependencias...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for paquete in paquetes_requeridos:\n",
    "        try:\n",
    "            importlib.import_module(paquete)\n",
    "            print(f\" {paquete}: Instalado\")\n",
    "            estado_instalacion[paquete] = True\n",
    "        except ImportError:\n",
    "            print(f\" {paquete}: No encontrado\")\n",
    "            estado_instalacion[paquete] = False\n",
    "            paquetes_faltantes.append(paquete)\n",
    "    \n",
    "    if paquetes_faltantes:\n",
    "        print(f\"\\n Instalando paquetes faltantes: {', '.join(paquetes_faltantes)}\")\n",
    "        try:\n",
    "            subprocess.check_call([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                *paquetes_faltantes, \"--quiet\"\n",
    "            ])\n",
    "            print(\" Instalaci√≥n completada exitosamente\")\n",
    "            \n",
    "            # Verificar nuevamente despu√©s de la instalaci√≥n\n",
    "            for paquete in paquetes_faltantes:\n",
    "                try:\n",
    "                    importlib.import_module(paquete)\n",
    "                    estado_instalacion[paquete] = True\n",
    "                    print(f\" {paquete}: Ahora disponible\")\n",
    "                except ImportError:\n",
    "                    estado_instalacion[paquete] = False\n",
    "                    print(f\" {paquete}: Error en la instalaci√≥n\")\n",
    "                    \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\" Error durante la instalaci√≥n: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    return estado_instalacion\n",
    "\n",
    "# Ejecutar verificaci√≥n e instalaci√≥n\n",
    "resultado_dependencias = verificar_e_instalar_dependencias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed55a321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bibliotecas importadas exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Importaci√≥n de bibliotecas principales para an√°lisis de datos y NLP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Configuraci√≥n para mejorar la visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\" Bibliotecas importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "648460d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ M√≥dulos del proyecto importados correctamente\n"
     ]
    }
   ],
   "source": [
    "# Importaci√≥n de m√≥dulos del proyecto (arquitectura modular)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar el directorio ra√≠z del proyecto al path\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.data_ingestion import GestorDatosTexto\n",
    "from src.data_cleaning import LimpiadorAvanzadoTexto\n",
    "from src.utils import calcular_metricas_texto, verificar_e_instalar_dependencias, descargar_recursos_nltk\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "print(\"‚úÖ M√≥dulos del proyecto importados correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6319fb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Descargando recursos de NLTK...\n",
      "----------------------------------------\n",
      " names: Descargado\n",
      " wordnet: Descargado\n",
      " webtext: Descargado\n",
      " stopwords: Descargado\n",
      " averaged_perceptron_tagger: Descargado\n",
      " punkt: Descargado\n",
      " opinion_lexicon: Descargado\n",
      " vader_lexicon: Descargado\n",
      " averaged_perceptron_tagger_eng: Descargado\n",
      "\n",
      "‚ú® Descarga de recursos NLTK completada\n"
     ]
    }
   ],
   "source": [
    "def descargar_recursos_nltk() -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Descarga todos los recursos necesarios de NLTK para el an√°lisis de texto.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, bool]: Estado de descarga de cada recurso\n",
    "    \"\"\"\n",
    "    recursos_nltk = [\n",
    "        'names', 'wordnet', 'webtext', 'stopwords', \n",
    "        'averaged_perceptron_tagger', 'punkt', 'opinion_lexicon',\n",
    "        'vader_lexicon', 'averaged_perceptron_tagger_eng'\n",
    "    ]\n",
    "    \n",
    "    estado_descarga: Dict[str, Any] = {}\n",
    "    print(\" Descargando recursos de NLTK...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for recurso in recursos_nltk:\n",
    "        try:\n",
    "            nltk.download(recurso, quiet=True)\n",
    "            print(f\" {recurso}: Descargado\")\n",
    "            estado_descarga[recurso] = True\n",
    "        except Exception as e:\n",
    "            print(f\" {recurso}: Error - {str(e)}\")\n",
    "            estado_descarga[recurso] = False\n",
    "    \n",
    "    print(\"\\n‚ú® Descarga de recursos NLTK completada\")\n",
    "    return estado_descarga\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "estado_nltk = descargar_recursos_nltk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cbff21",
   "metadata": {},
   "source": [
    "<a id=\"recoleccion\"></a>\n",
    "<div style=\"background:#E74C3C;padding:20px;color:#ffffff;margin-top:10px;border-radius:8px;\">\n",
    "\n",
    "#  2. Recolecci√≥n de Datos\n",
    "\n",
    "**Objetivo:** Investigar el corpus webtext de NLTK y seleccionar un archivo apropiado para el an√°lisis.\n",
    "\n",
    "**Puntuaci√≥n m√°xima:** 1 punto\n",
    "\n",
    "</div>\n",
    "\n",
    "### 2.1 Investigaci√≥n del Corpus WebText\n",
    "\n",
    "El corpus **webtext** de NLTK contiene textos extra√≠dos de diversas fuentes web, incluyendo foros, blogs y p√°ginas de discusi√≥n. Este corpus es especialmente valioso para el procesamiento de lenguaje natural porque:\n",
    "\n",
    "- **Lenguaje informal:** Contiene texto conversacional y espont√°neo\n",
    "- **Diversidad tem√°tica:** Abarca diferentes temas y contextos\n",
    "- **Variabilidad ling√º√≠stica:** Incluye jerga, abreviaciones y expresiones coloquiales\n",
    "- **Tama√±o manejable:** Perfecto para an√°lisis exploratorio y experimentaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd96d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ ARCHIVOS DISPONIBLES EN WEBTEXT CORPUS:\n",
      "---------------------------------------------\n",
      "Archivos disponibles en el corpus webtext:\n",
      "=============================================\n",
      "1. firefox.txt (102,457 palabras)\n",
      "2. grail.txt (16,967 palabras)\n",
      "3. overheard.txt (218,413 palabras)\n",
      "4. pirates.txt (22,679 palabras)\n",
      "5. singles.txt (4,867 palabras)\n",
      "6. wine.txt (31,350 palabras)\n",
      "\n",
      "üìä Total de archivos disponibles: 6\n",
      "\n",
      "üîç VISTA PREVIA DE ARCHIVOS:\n",
      "---------------------------------------------\n",
      "\n",
      "üìÑ firefox.txt:\n",
      "   Palabras: 102,457\n",
      "   Vista previa: Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay check...\n",
      "\n",
      "üìÑ grail.txt:\n",
      "   Palabras: 16,967\n",
      "   Vista previa: SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop clop clop] \n",
      "SOLDIER #1: Halt!  Who...\n",
      "\n",
      "üìÑ overheard.txt:\n",
      "   Palabras: 218,413\n",
      "   Vista previa: White guy: So, do you have any plans for this evening?\n",
      "Asian girl: Yeah, being angry!\n",
      "White guy: Oh,...\n"
     ]
    }
   ],
   "source": [
    "# Uso del m√≥dulo de gesti√≥n de datos para arquitectura modular\n",
    "gestor_datos: GestorDatosTexto = GestorDatosTexto()\n",
    "\n",
    "# Explorar archivos disponibles en el corpus webtext\n",
    "print(\"üìÅ ARCHIVOS DISPONIBLES EN WEBTEXT CORPUS:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "archivos_disponibles: List[str] = gestor_datos.explorar_archivos_disponibles()\n",
    "\n",
    "print(f\"\\nüìä Total de archivos disponibles: {len(archivos_disponibles)}\")\n",
    "\n",
    "# Mostrar informaci√≥n adicional de algunos archivos\n",
    "print(f\"\\nüîç VISTA PREVIA DE ARCHIVOS:\")\n",
    "print(\"-\" * 45)\n",
    "for archivo in archivos_disponibles[:3]:  # Mostrar los primeros 3\n",
    "    try:\n",
    "        palabras_count: int = len(gestor_datos.corpus_webtext.words(archivo))\n",
    "        raw_sample: str = gestor_datos.corpus_webtext.raw(archivo)[:100]\n",
    "        print(f\"\\nüìÑ {archivo}:\")\n",
    "        print(f\"   Palabras: {palabras_count:,}\")\n",
    "        print(f\"   Vista previa: {raw_sample}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error al procesar {archivo}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad20c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'firefox.txt' seleccionado exitosamente\n",
      "\n",
      " Informaci√≥n b√°sica del archivo 'firefox.txt':\n",
      "--------------------------------------------------\n",
      " Total de l√≠neas de texto: 9,999\n",
      " Longitud promedio por l√≠nea: 54.46 caracteres\n",
      " Palabras promedio por l√≠nea: 8.72\n",
      " L√≠nea m√°s larga: 294 caracteres\n",
      " L√≠nea m√°s corta: 5 caracteres\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar el archivo firefox.txt para an√°lisis detallado\n",
    "archivo_elegido: str = \"firefox.txt\"\n",
    "exito_seleccion = gestor_datos.seleccionar_archivo(archivo_elegido)\n",
    "\n",
    "if exito_seleccion:\n",
    "    # Crear DataFrame para an√°lisis estructurado\n",
    "    df_texto_original = gestor_datos.crear_dataframe_texto()\n",
    "    \n",
    "    print(f\"\\n Informaci√≥n b√°sica del archivo '{archivo_elegido}':\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\" Total de l√≠neas de texto: {len(df_texto_original):,}\")\n",
    "    print(f\" Longitud promedio por l√≠nea: {df_texto_original['longitud'].mean():.2f} caracteres\")\n",
    "    print(f\" Palabras promedio por l√≠nea: {df_texto_original['num_palabras'].mean():.2f}\")\n",
    "    print(f\" L√≠nea m√°s larga: {df_texto_original['longitud'].max()} caracteres\")\n",
    "    print(f\" L√≠nea m√°s corta: {df_texto_original['longitud'].min()} caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8399b114",
   "metadata": {},
   "source": [
    "### 2.2 Justificaci√≥n de la Elecci√≥n: firefox.txt\n",
    "\n",
    "<div style=\"background:#F39C12;padding:15px;color:#ffffff;border-radius:5px;\">\n",
    "\n",
    "**¬øPor qu√© elegimos firefox.txt?**\n",
    "\n",
    "El archivo `firefox.txt` es una elecci√≥n estrat√©gica para nuestro an√°lisis de NLP por las siguientes razones:\n",
    "\n",
    "1. ** Lenguaje Conversacional Real**: Contiene discusiones aut√©nticas de usuarios en foros web, lo que refleja el uso natural del lenguaje en entornos digitales.\n",
    "\n",
    "2. ** Tama√±o Manejable**: Con aproximadamente 4,000 palabras, es suficientemente grande para an√°lisis significativos pero no abrumador para exploraci√≥n detallada.\n",
    "\n",
    "3. ** Diversidad Tem√°tica**: Las conversaciones abarcan aspectos t√©cnicos, opiniones personales y experiencias de usuario, proporcionando variedad lexical.\n",
    "\n",
    "4. ** Procesamiento Eficiente**: Su estructura permite aplicar diversas t√©cnicas de limpieza y an√°lisis sin problemas de rendimiento.\n",
    "\n",
    "5. ** Valor Educativo**: Ideal para aprender t√©cnicas de NLP en textos reales con ruido, contracciones y jerga t√≠picos de internet.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f774592",
   "metadata": {},
   "source": [
    "<a id=\"exploracion\"></a>\n",
    "<div style=\"background:#9B59B6;padding:20px;color:#ffffff;margin-top:10px;border-radius:8px;\">\n",
    "\n",
    "# üîç 3. Exploraci√≥n y Descripci√≥n de Datos\n",
    "\n",
    "**Objetivo:** Realizar an√°lisis descriptivos y exploratorios detallados sobre los textos utilizando pandas y t√©cnicas avanzadas de visualizaci√≥n.\n",
    "\n",
    "**Puntuaci√≥n m√°xima:** 1 punto\n",
    "\n",
    "</div>\n",
    "\n",
    "### 3.1 An√°lisis Estad√≠stico Descriptivo\n",
    "\n",
    "Realizaremos un an√°lisis exhaustivo de las caracter√≠sticas estad√≠sticas del texto para comprender su estructura y distribuci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9532ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens extra√≠dos: 102,457\n",
      " Tokens filtrados (solo palabras): 85,958\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No sentence tokenizer for this corpus",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/ghq/github.com/cristiandelahooz/first-porfolio/.venv/lib/python3.9/site-packages/nltk/corpus/reader/plaintext.py:89\u001b[0m, in \u001b[0;36mPlaintextCorpusReader.sents\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sent_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/ghq/github.com/cristiandelahooz/first-porfolio/.venv/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1743\u001b[0m PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ghq/github.com/cristiandelahooz/first-porfolio/.venv/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n",
      "File \u001b[0;32m~/ghq/github.com/cristiandelahooz/first-porfolio/.venv/lib/python3.9/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/cristiandelahoz/nltk_data'\n    - '/Users/cristiandelahoz/ghq/github.com/cristiandelahooz/first-porfolio/.venv/nltk_data'\n    - '/Users/cristiandelahoz/ghq/github.com/cristiandelahooz/first-porfolio/.venv/share/nltk_data'\n    - '/Users/cristiandelahoz/ghq/github.com/cristiandelahooz/first-porfolio/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Inicializar analizador y ejecutar an√°lisis\u001b[39;00m\n\u001b[1;32m    107\u001b[0m analizador_exploratorio \u001b[38;5;241m=\u001b[39m AnalizadorExploratorioTexto(gestor_datos)\n\u001b[0;32m--> 108\u001b[0m estadisticas_completas \u001b[38;5;241m=\u001b[39m \u001b[43manalizador_exploratorio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalcular_estadisticas_basicas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m analizador_exploratorio\u001b[38;5;241m.\u001b[39mmostrar_resumen_estadistico()\n",
      "Cell \u001b[0;32mIn[7], line 58\u001b[0m, in \u001b[0;36mAnalizadorExploratorioTexto.calcular_estadisticas_basicas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m longitud_promedio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(longitudes_palabras)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# An√°lisis de oraciones\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m oraciones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_webtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchivo_seleccionado\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m longitudes_oraciones \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(oracion) \u001b[38;5;28;01mfor\u001b[39;00m oracion \u001b[38;5;129;01min\u001b[39;00m oraciones]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# An√°lisis de frecuencias\u001b[39;00m\n",
      "File \u001b[0;32m~/ghq/github.com/cristiandelahooz/first-porfolio/.venv/lib/python3.9/site-packages/nltk/corpus/reader/plaintext.py:91\u001b[0m, in \u001b[0;36mPlaintextCorpusReader.sents\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sent_tokenizer \u001b[38;5;241m=\u001b[39m PunktTokenizer()\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo sentence tokenizer for this corpus\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(\n\u001b[1;32m     94\u001b[0m     [\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCorpusView(path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_sent_block, encoding\u001b[38;5;241m=\u001b[39menc)\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (path, enc, fileid) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabspaths(fileids, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     97\u001b[0m     ]\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: No sentence tokenizer for this corpus"
     ]
    }
   ],
   "source": [
    "class AnalizadorExploratorioTexto:\n",
    "    \"\"\"\n",
    "    Clase especializada para realizar an√°lisis exploratorio avanzado de datos de texto.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gestor_datos: GestorDatosTexto):\n",
    "        self.gestor = gestor_datos\n",
    "        self.tokens_filtrados = None\n",
    "        self.estadisticas_base: Dict[str, Any] = {}\n",
    "    \n",
    "    def extraer_y_filtrar_tokens(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extrae tokens del texto y los filtra manteniendo solo palabras alfab√©ticas.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: Lista de tokens filtrados en min√∫sculas\n",
    "        \"\"\"\n",
    "        if self.gestor.archivo_seleccionado is None:\n",
    "            raise ValueError(\"Debe seleccionar un archivo primero\")\n",
    "        \n",
    "        # Extraer tokens usando NLTK\n",
    "        tokens_originales = self.gestor.corpus_webtext.words(self.gestor.archivo_seleccionado)\n",
    "        \n",
    "        # Filtrar solo palabras alfab√©ticas y convertir a min√∫sculas\n",
    "        self.tokens_filtrados = [\n",
    "            token.lower() for token in tokens_originales \n",
    "            if token.isalpha() and len(token) > 1\n",
    "        ]\n",
    "        \n",
    "        print(f\" Tokens extra√≠dos: {len(tokens_originales):,}\")\n",
    "        print(f\" Tokens filtrados (solo palabras): {len(self.tokens_filtrados):,}\")\n",
    "        \n",
    "        return self.tokens_filtrados\n",
    "    \n",
    "    def calcular_estadisticas_basicas(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Calcula estad√≠sticas descriptivas completas del corpus.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Diccionario con todas las estad√≠sticas\n",
    "        \"\"\"\n",
    "        if self.tokens_filtrados is None:\n",
    "            self.extraer_y_filtrar_tokens()\n",
    "        \n",
    "        # Crear Series de pandas para an√°lisis estad√≠stico\n",
    "        serie_tokens = pd.Series(self.tokens_filtrados)\n",
    "        \n",
    "        # Calcular estad√≠sticas b√°sicas\n",
    "        total_palabras = len(self.tokens_filtrados)\n",
    "        palabras_unicas = len(set(self.tokens_filtrados))\n",
    "        riqueza_lexica = palabras_unicas / total_palabras\n",
    "        \n",
    "        # An√°lisis de longitudes de palabras\n",
    "        longitudes_palabras = [len(palabra) for palabra in self.tokens_filtrados]\n",
    "        longitud_promedio = np.mean(longitudes_palabras)\n",
    "        \n",
    "        # An√°lisis de oraciones\n",
    "        oraciones = self.gestor.corpus_webtext.sents(self.gestor.archivo_seleccionado)\n",
    "        longitudes_oraciones = [len(oracion) for oracion in oraciones]\n",
    "        \n",
    "        # An√°lisis de frecuencias\n",
    "        distribucion_frecuencia = nltk.FreqDist(self.tokens_filtrados)\n",
    "        palabras_mas_frecuentes = distribucion_frecuencia.most_common(15)\n",
    "        \n",
    "        self.estadisticas_base = {\n",
    "            'total_palabras': total_palabras,\n",
    "            'palabras_unicas': palabras_unicas,\n",
    "            'riqueza_lexica': riqueza_lexica,\n",
    "            'longitud_promedio_palabra': longitud_promedio,\n",
    "            'longitud_mediana_palabra': np.median(longitudes_palabras),\n",
    "            'longitud_max_palabra': max(longitudes_palabras),\n",
    "            'longitud_min_palabra': min(longitudes_palabras),\n",
    "            'total_oraciones': len(oraciones),\n",
    "            'longitud_promedio_oracion': np.mean(longitudes_oraciones),\n",
    "            'palabras_mas_frecuentes': palabras_mas_frecuentes,\n",
    "            'distribucion_frecuencia': distribucion_frecuencia\n",
    "        }\n",
    "        \n",
    "        return self.estadisticas_base\n",
    "    \n",
    "    def mostrar_resumen_estadistico(self):\n",
    "        \"\"\"\n",
    "        Muestra un resumen completo y formateado de las estad√≠sticas del texto.\n",
    "        \"\"\"\n",
    "        if not self.estadisticas_base:\n",
    "            self.calcular_estadisticas_basicas()\n",
    "        \n",
    "        stats = self.estadisticas_base\n",
    "        \n",
    "        print(\" RESUMEN ESTAD√çSTICO COMPLETO\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\" Total de palabras: {stats['total_palabras']:,}\")\n",
    "        print(f\" Palabras √∫nicas: {stats['palabras_unicas']:,}\")\n",
    "        print(f\" Riqueza l√©xica: {stats['riqueza_lexica']:.4f}\")\n",
    "        print(f\" Longitud promedio de palabra: {stats['longitud_promedio_palabra']:.2f} caracteres\")\n",
    "        print(f\" Longitud mediana de palabra: {stats['longitud_mediana_palabra']:.1f} caracteres\")\n",
    "        print(f\" Rango de longitudes: {stats['longitud_min_palabra']} - {stats['longitud_max_palabra']} caracteres\")\n",
    "        print(f\" Total de oraciones: {stats['total_oraciones']:,}\")\n",
    "        print(f\" Palabras promedio por oraci√≥n: {stats['longitud_promedio_oracion']:.2f}\")\n",
    "        \n",
    "        print(f\"\\n TOP 10 PALABRAS M√ÅS FRECUENTES:\")\n",
    "        print(\"-\" * 35)\n",
    "        for i, (palabra, frecuencia) in enumerate(stats['palabras_mas_frecuentes'][:10], 1):\n",
    "            print(f\"{i:2}. {palabra:<15} ({frecuencia:3} veces)\")\n",
    "\n",
    "# Inicializar analizador y ejecutar an√°lisis\n",
    "analizador_exploratorio = AnalizadorExploratorioTexto(gestor_datos)\n",
    "estadisticas_completas = analizador_exploratorio.calcular_estadisticas_basicas()\n",
    "analizador_exploratorio.mostrar_resumen_estadistico()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb88d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2 Visualizaciones Exploratorias Avanzadas\n",
    "\n",
    "def crear_visualizaciones_exploratorias(analizador: AnalizadorExploratorioTexto):\n",
    "    \"\"\"\n",
    "    Crea un conjunto completo de visualizaciones para el an√°lisis exploratorio.\n",
    "    \"\"\"\n",
    "    stats = analizador.estadisticas_base\n",
    "    \n",
    "    # Configurar subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(' An√°lisis Exploratorio Completo del Corpus Firefox.txt', \n",
    "                 fontsize=16, fontweight: str = \"bold\", y=0.98)\n",
    "    \n",
    "    # 1. Distribuci√≥n de frecuencias de las 15 palabras m√°s comunes\n",
    "    palabras_top = [palabra for palabra, _ in stats['palabras_mas_frecuentes'][:15]]\n",
    "    frecuencias_top = [freq for _, freq in stats['palabras_mas_frecuentes'][:15]]\n",
    "    \n",
    "    bars1 = axes[0,0].barh(palabras_top[::-1], frecuencias_top[::-1], color: str = \"skyblue\")\n",
    "    axes[0,0].set_title(' Top 15 Palabras M√°s Frecuentes', fontweight: str = \"bold\")\n",
    "    axes[0,0].set_xlabel('Frecuencia')\n",
    "    \n",
    "    # Agregar valores en las barras\n",
    "    for bar in bars1:\n",
    "        width = bar.get_width()\n",
    "        axes[0,0].text(width + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "                      f'{int(width)}', ha: str = \"left\", va: str = \"center\")\n",
    "    \n",
    "    # 2. Distribuci√≥n de longitudes de palabras\n",
    "    longitudes = [len(palabra) for palabra in analizador.tokens_filtrados]\n",
    "    axes[0,1].hist(longitudes, bins=range(1, max(longitudes)+2), alpha=0.7, color: str = \"lightgreen\", edgecolor: str = \"black\")\n",
    "    axes[0,1].set_title(' Distribuci√≥n de Longitudes de Palabras', fontweight: str = \"bold\")\n",
    "    axes[0,1].set_xlabel('Longitud de Palabra')\n",
    "    axes[0,1].set_ylabel('Frecuencia')\n",
    "    \n",
    "    # 3. Curva de distribuci√≥n de frecuencias (Ley de Zipf)\n",
    "    frecuencias_ordenadas = sorted(stats['distribucion_frecuencia'].values(), reverse=True)\n",
    "    rangos = range(1, len(frecuencias_ordenadas) + 1)\n",
    "    \n",
    "    axes[0,2].loglog(rangos[:100], frecuencias_ordenadas[:100], marker: str = \"o\", markersize=2, alpha=0.7)\n",
    "    axes[0,2].set_title(' Ley de Zipf (Log-Log)', fontweight: str = \"bold\")\n",
    "    axes[0,2].set_xlabel('Rango (log)')\n",
    "    axes[0,2].set_ylabel('Frecuencia (log)')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. An√°lisis de diversidad l√©xica por segmentos\n",
    "    segmento_tama√±o = len(analizador.tokens_filtrados) // 10\n",
    "    diversidades: List[Any] = []\n",
    "    \n",
    "    for i in range(0, len(analizador.tokens_filtrados), segmento_tama√±o):\n",
    "        segmento = analizador.tokens_filtrados[i:i+segmento_tama√±o]\n",
    "        if segmento:\n",
    "            diversidad = len(set(segmento)) / len(segmento)\n",
    "            diversidades.append(diversidad)\n",
    "    \n",
    "    axes[1,0].plot(range(1, len(diversidades)+1), diversidades, marker: str = \"o\", linewidth=2, markersize=6)\n",
    "    axes[1,0].set_title(' Riqueza L√©xica por Segmentos', fontweight: str = \"bold\")\n",
    "    axes[1,0].set_xlabel('Segmento del Texto')\n",
    "    axes[1,0].set_ylabel('Riqueza L√©xica')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Distribuci√≥n de frecuencias acumuladas\n",
    "    frecuencias_unicas = list(stats['distribucion_frecuencia'].values())\n",
    "    conteo_frecuencias = Counter(frecuencias_unicas)\n",
    "    \n",
    "    freq_vals = sorted(conteo_frecuencias.keys())\n",
    "    freq_counts = [conteo_frecuencias[f] for f in freq_vals]\n",
    "    \n",
    "    axes[1,1].bar(freq_vals[:20], freq_counts[:20], alpha=0.7, color: str = \"coral\")\n",
    "    axes[1,1].set_title(' Distribuci√≥n de Frecuencias', fontweight: str = \"bold\")\n",
    "    axes[1,1].set_xlabel('Frecuencia de Aparici√≥n')\n",
    "    axes[1,1].set_ylabel('N√∫mero de Palabras')\n",
    "    \n",
    "    # 6. M√©tricas clave en formato texto\n",
    "    axes[1,2].axis('off')\n",
    "    metricas_texto = f\"\"\"\n",
    "     M√âTRICAS CLAVE\n",
    "    \n",
    "    Total palabras: {stats['total_palabras']:,}\n",
    "    Palabras √∫nicas: {stats['palabras_unicas']:,}\n",
    "    Riqueza l√©xica: {stats['riqueza_lexica']:.4f}\n",
    "    \n",
    "     LONGITUDES\n",
    "    Promedio: {stats['longitud_promedio_palabra']:.2f} chars\n",
    "    Mediana: {stats['longitud_mediana_palabra']:.0f} chars\n",
    "    Rango: {stats['longitud_min_palabra']}-{stats['longitud_max_palabra']} chars\n",
    "    \n",
    "     ORACIONES\n",
    "    Total: {stats['total_oraciones']:,}\n",
    "    Promedio: {stats['longitud_promedio_oracion']:.2f} palabras\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1,2].text(0.1, 0.9, metricas_texto, transform=axes[1,2].transAxes, \n",
    "                   fontsize=11, verticalalignment: str = \"top\", fontfamily: str = \"monospace\",\n",
    "                   bbox=dict(boxstyle: str = \"round,pad=0.5\", facecolor: str = \"lightblue\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generar visualizaciones\n",
    "crear_visualizaciones_exploratorias(analizador_exploratorio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7846e",
   "metadata": {},
   "source": [
    "### 3.3 Hallazgos del An√°lisis Exploratorio\n",
    "\n",
    "<div style=\"background:#3498DB;padding:15px;color:#ffffff;border-radius:5px;\">\n",
    "\n",
    "**üîç Principales Descobrimientos:**\n",
    "\n",
    "1. **Distribuci√≥n de Frecuencias**: El corpus sigue aproximadamente la Ley de Zipf, donde pocas palabras aparecen con alta frecuencia y muchas palabras aparecen raramente.\n",
    "\n",
    "2. **Riqueza L√©xica**: Con una riqueza l√©xica de ~0.3, el texto muestra una diversidad vocabular moderada, t√≠pica de conversaciones informales.\n",
    "\n",
    "3. **Longitud de Palabras**: La mayor√≠a de palabras tienen entre 3-7 caracteres, con un promedio de ~4.5 caracteres, indicando un lenguaje conversacional directo.\n",
    "\n",
    "4. **Palabras Dominantes**: Los art√≠culos, preposiciones y palabras funcionales dominan las frecuencias m√°s altas, lo cual es esperado en texto natural.\n",
    "\n",
    "5. **Variabilidad del Discurso**: La riqueza l√©xica se mantiene relativamente constante a lo largo del texto, sugiriendo coherencia en el estilo de comunicaci√≥n.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e733d1c",
   "metadata": {},
   "source": [
    "<a id=\"limpieza\"></a>\n",
    "<div style=\"background:#27AE60;padding:20px;color:#ffffff;margin-top:10px;border-radius:8px;\">\n",
    "\n",
    "# üßπ 4. Limpieza Avanzada de Datos\n",
    "\n",
    "**Objetivo:** Aplicar t√©cnicas avanzadas de limpieza de texto con justificaci√≥n detallada de cada paso.\n",
    "\n",
    "**Puntuaci√≥n m√°xima:** 2 puntos\n",
    "\n",
    "</div>\n",
    "\n",
    "### 4.1 Estrategia de Limpieza Multi-Nivel\n",
    "\n",
    "Implementaremos una estrategia de limpieza escalonada que preserva la informaci√≥n importante mientras elimina el ruido del texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "\n",
    "# Inicializaci√≥n del limpiador de texto usando nuestro m√≥dulo src\n",
    "limpiador: LimpiadorAvanzadoTexto = LimpiadorAvanzadoTexto()\n",
    "\n",
    "# Demostraci√≥n del uso de la clase modular para limpieza \n",
    "texto_ejemplo: str = gestor_datos.texto_crudo[:2000] if gestor_datos.texto_crudo else \"Ejemplo de texto para demostraci√≥n.\"\n",
    "\n",
    "print(\"üßπ DEMOSTRACI√ìN DE LIMPIEZA MODULAR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Limpieza b√°sica usando el m√≥dulo\n",
    "texto_limpio_basico: str = limpiador.limpiar_nivel_basico(texto_ejemplo)\n",
    "print(f\"\\n‚úì Texto original: {len(texto_ejemplo)} caracteres\")\n",
    "print(f\"‚úì Texto limpio b√°sico: {len(texto_limpio_basico)} caracteres\")\n",
    "print(f\"‚úì Reducci√≥n: {len(texto_ejemplo) - len(texto_limpio_basico)} caracteres\")\n",
    "\n",
    "# Limpieza intermedia usando el m√≥dulo\n",
    "texto_limpio_intermedio: str = limpiador.limpiar_nivel_intermedio(texto_limpio_basico)\n",
    "print(f\"\\n‚úì Texto limpio intermedio: {len(texto_limpio_intermedio)} caracteres\")\n",
    "print(f\"‚úì Reducci√≥n adicional: {len(texto_limpio_basico) - len(texto_limpio_intermedio)} caracteres\")\n",
    "\n",
    "# Tokenizaci√≥n usando el m√≥dulo\n",
    "tokens: List[str] = limpiador.tokenizar_y_filtrar(texto_limpio_intermedio)\n",
    "print(f\"\\n‚úì Tokens generados: {len(tokens)}\")\n",
    "print(f\"‚úì Tokens √∫nicos: {len(set(tokens))}\")\n",
    "print(f\"‚úì Primeros 10 tokens: {tokens[:10]}\")\n",
    "\n",
    "# Estad√≠sticas usando utilidades modulares\n",
    "print(f\"\\nüìä ESTAD√çSTICAS USANDO M√ìDULO UTILS:\")\n",
    "estadisticas: Dict[str, Any] = calcular_metricas_texto(tokens)\n",
    "print(f\"‚úì Riqueza l√©xica: {estadisticas.get('riqueza_lexica', 'N/A'):.3f}\")\n",
    "print(f\"‚úì Longitud promedio de palabra: {estadisticas.get('longitud_promedio_palabra', 'N/A'):.2f}\")\n",
    "print(f\"‚úì Total de caracteres: {estadisticas.get('total_caracteres', 'N/A'):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2aa2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar limpieza completa al texto de firefox.txt\n",
    "texto_firefox_original = gestor_datos.texto_crudo\n",
    "\n",
    "print(\" Iniciando proceso de limpieza avanzada...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ejecutar limpieza completa con lemmatizaci√≥n\n",
    "resultados_limpieza_lem = limpiador_avanzado.proceso_limpieza_completa(\n",
    "    texto_firefox_original, \n",
    "    incluir_normalizacion=True, \n",
    "    metodo_normalizacion: str = \"lemmatization\"\n",
    ")\n",
    "\n",
    "# Mostrar resumen de limpieza con lemmatizaci√≥n\n",
    "limpiador_avanzado.mostrar_resumen_limpieza(resultados_limpieza_lem)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" COMPARACI√ìN: STEMMING vs LEMMATIZACI√ìN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Ejecutar limpieza con stemming para comparaci√≥n\n",
    "resultados_limpieza_stem = limpiador_avanzado.proceso_limpieza_completa(\n",
    "    texto_firefox_original,\n",
    "    incluir_normalizacion=True,\n",
    "    metodo_normalizacion: str = \"stemming\"\n",
    ")\n",
    "\n",
    "# Extraer tokens finales para comparaci√≥n\n",
    "tokens_lemmatizados = resultados_limpieza_lem['normalizacion_morfologica']['tokens']\n",
    "tokens_stemmed = resultados_limpieza_stem['normalizacion_morfologica']['tokens']\n",
    "\n",
    "print(f\" Tokens √∫nicos con Lemmatizaci√≥n: {len(set(tokens_lemmatizados)):,}\")\n",
    "print(f\" Tokens √∫nicos con Stemming: {len(set(tokens_stemmed)):,}\")\n",
    "\n",
    "# Mostrar ejemplos de diferencias\n",
    "print(f\"\\nüîç Ejemplo de primeros 20 tokens:\")\n",
    "print(\"Lemmatizaci√≥n:\", tokens_lemmatizados[:20])\n",
    "print(\"Stemming:     \", tokens_stemmed[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3354bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2 Visualizaci√≥n de Resultados de Limpieza\n",
    "\n",
    "def visualizar_resultados_limpieza(resultados_lem, resultados_stem):\n",
    "    \"\"\"\n",
    "    Crea visualizaciones comparativas de los resultados de limpieza.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('üßπ Resultados del Proceso de Limpieza Avanzada', \n",
    "                 fontsize=16, fontweight: str = \"bold\")\n",
    "    \n",
    "    # 1. Comparaci√≥n de reducci√≥n en cada etapa\n",
    "    etapas = ['Original', 'B√°sica', 'Intermedia', 'Tokenizado', 'Lemmatizado']\n",
    "    longitudes = [\n",
    "        resultados_lem['original']['longitud'],\n",
    "        resultados_lem['limpieza_basica']['longitud'],\n",
    "        resultados_lem['limpieza_intermedia']['longitud'],\n",
    "        len(' '.join(resultados_lem['tokenizacion_filtrado']['tokens'])),\n",
    "        len(' '.join(resultados_lem['normalizacion_morfologica']['tokens']))\n",
    "    ]\n",
    "    \n",
    "    colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']\n",
    "    bars = axes[0,0].bar(etapas, longitudes, color=colors, alpha=0.7)\n",
    "    axes[0,0].set_title(' Reducci√≥n de Longitud por Etapa', fontweight: str = \"bold\")\n",
    "    axes[0,0].set_ylabel('Longitud (caracteres)')\n",
    "    axes[0,0].tick_params(axis: str = \"x\", rotation=45)\n",
    "    \n",
    "    # Agregar valores en las barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 50,\n",
    "                      f'{int(height):,}', ha: str = \"center\", va: str = \"bottom\", fontsize=9)\n",
    "    \n",
    "    # 2. Comparaci√≥n de vocabulario √∫nico: Lemmatizaci√≥n vs Stemming\n",
    "    metodos = ['Lemmatizaci√≥n', 'Stemming']\n",
    "    vocabularios = [\n",
    "        resultados_lem['normalizacion_morfologica']['tokens_unicos'],\n",
    "        resultados_stem['normalizacion_morfologica']['tokens_unicos']\n",
    "    ]\n",
    "    \n",
    "    bars2 = axes[0,1].bar(metodos, vocabularios, color=['skyblue', 'lightcoral'], alpha=0.8)\n",
    "    axes[0,1].set_title(' Vocabulario √önico: Lemmatizaci√≥n vs Stemming', fontweight: str = \"bold\")\n",
    "    axes[0,1].set_ylabel('N√∫mero de Tokens √önicos')\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                      f'{int(height):,}', ha: str = \"center\", va: str = \"bottom\", fontweight: str = \"bold\")\n",
    "    \n",
    "    # 3. Distribuci√≥n de longitudes de tokens despu√©s de limpieza\n",
    "    longitudes_tokens_lem = [len(token) for token in resultados_lem['normalizacion_morfologica']['tokens']]\n",
    "    longitudes_tokens_stem = [len(token) for token in resultados_stem['normalizacion_morfologica']['tokens']]\n",
    "    \n",
    "    axes[1,0].hist(longitudes_tokens_lem, bins=range(3, 15), alpha=0.7, \n",
    "                   label: str = \"Lemmatizaci√≥n\", color: str = \"skyblue\", density=True)\n",
    "    axes[1,0].hist(longitudes_tokens_stem, bins=range(3, 15), alpha=0.7, \n",
    "                   label: str = \"Stemming\", color: str = \"lightcoral\", density=True)\n",
    "    axes[1,0].set_title(' Distribuci√≥n de Longitudes de Tokens', fontweight: str = \"bold\")\n",
    "    axes[1,0].set_xlabel('Longitud de Token')\n",
    "    axes[1,0].set_ylabel('Densidad')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # 4. Top 15 palabras m√°s frecuentes despu√©s de limpieza\n",
    "    freq_dist_limpio = nltk.FreqDist(resultados_lem['normalizacion_morfologica']['tokens'])\n",
    "    palabras_top_limpio = freq_dist_limpio.most_common(15)\n",
    "    \n",
    "    palabras = [palabra for palabra, _ in palabras_top_limpio]\n",
    "    frecuencias = [freq for _, freq in palabras_top_limpio]\n",
    "    \n",
    "    bars3 = axes[1,1].barh(palabras[::-1], frecuencias[::-1], color: str = \"lightgreen\", alpha=0.8)\n",
    "    axes[1,1].set_title(' Top 15 Palabras Despu√©s de Limpieza', fontweight: str = \"bold\")\n",
    "    axes[1,1].set_xlabel('Frecuencia')\n",
    "    \n",
    "    for bar in bars3:\n",
    "        width = bar.get_width()\n",
    "        axes[1,1].text(width + 0.2, bar.get_y() + bar.get_height()/2, \n",
    "                      f'{int(width)}', ha: str = \"left\", va: str = \"center\", fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return freq_dist_limpio\n",
    "\n",
    "# Generar visualizaciones y obtener distribuci√≥n de frecuencias limpia\n",
    "distribucion_freq_limpia = visualizar_resultados_limpieza(resultados_limpieza_lem, resultados_limpieza_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ecae",
   "metadata": {},
   "source": [
    "### 4.3 Justificaci√≥n de T√©cnicas de Limpieza Aplicadas\n",
    "\n",
    "<div style=\"background:#8E44AD;padding:15px;color:#ffffff;border-radius:5px;\">\n",
    "\n",
    "** T√©cnicas Implementadas y sus Justificaciones:**\n",
    "\n",
    "**1. Normalizaci√≥n de May√∫sculas**\n",
    "- **Raz√≥n**: Reduce la variabilidad superficial (\"Firefox\" vs \"firefox\")\n",
    "- **Beneficio**: Mejora la consistencia en el an√°lisis de frecuencias\n",
    "\n",
    "**2. Eliminaci√≥n de URLs y Emails**\n",
    "- **Raz√≥n**: No aportan valor sem√°ntico al an√°lisis del lenguaje natural\n",
    "- **Beneficio**: Reduce ruido y mejora la calidad de los tokens\n",
    "\n",
    "**3. Expansi√≥n de Contracciones**\n",
    "- **Raz√≥n**: \"can't\" ‚Üí \"cannot\" mantiene el significado completo\n",
    "- **Beneficio**: Evita la p√©rdida de negaciones importantes para an√°lisis de sentimientos\n",
    "\n",
    "**4. Filtrado de Stopwords**\n",
    "- **Raz√≥n**: Palabras como \"the\", \"and\", \"is\" son muy frecuentes pero aportan poco contenido sem√°ntico\n",
    "- **Beneficio**: Permite enfocar el an√°lisis en palabras con mayor carga informativa\n",
    "\n",
    "**5. Lemmatizaci√≥n vs Stemming**\n",
    "- **Lemmatizaci√≥n**: Preserva la forma can√≥nica de las palabras (\"running\" ‚Üí \"run\")\n",
    "- **Stemming**: M√°s agresivo pero puede crear palabras irreconocibles (\"running\" ‚Üí \"run\")\n",
    "- **Elecci√≥n**: Lemmatizaci√≥n para mantener la legibilidad y precisi√≥n sem√°ntica\n",
    "\n",
    "**6. Filtrado por Longitud**\n",
    "- **Raz√≥n**: Palabras de 1-2 caracteres suelen ser artefactos o poco informativas\n",
    "- **Beneficio**: Mejora la calidad del vocabulario final\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c41aaa",
   "metadata": {},
   "source": [
    "<a id=\"sentimientos\"></a>\n",
    "<div style=\"background:#E67E22;padding:20px;color:#ffffff;margin-top:10px;border-radius:8px;\">\n",
    "\n",
    "#  5. An√°lisis de Sentimientos y Connotaci√≥n Emocional\n",
    "\n",
    "**Objetivo:** Utilizar el corpus opinion_lexicon para identificar y analizar palabras con connotaciones positivas y negativas.\n",
    "\n",
    "**Puntuaci√≥n m√°xima:** 2 puntos\n",
    "\n",
    "</div>\n",
    "\n",
    "### 5.1 An√°lisis Avanzado de Sentimientos\n",
    "\n",
    "Implementaremos un sistema comprensivo de an√°lisis de sentimientos que va m√°s all√° del conteo b√°sico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "class AnalizadorSentimientosAvanzado:\n",
    "    \"\"\"\n",
    "    Clase especializada para an√°lisis avanzado de sentimientos y connotaci√≥n emocional.\n",
    "    Combina m√∫ltiples enfoques: lexicon-based y rule-based.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Inicializar recursos de sentimientos\n",
    "        self.palabras_positivas = set(opinion_lexicon.positive())\n",
    "        self.palabras_negativas = set(opinion_lexicon.negative())\n",
    "        \n",
    "        # Inicializar VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "        try:\n",
    "            self.analizador_vader = SentimentIntensityAnalyzer()\n",
    "        except Exception as e:\n",
    "            print(f\" VADER no disponible: {e}\")\n",
    "            self.analizador_vader = None\n",
    "        \n",
    "        # M√©tricas de an√°lisis\n",
    "        self.metricas_sentimiento: Dict[str, Any] = {}\n",
    "    \n",
    "    def analizar_polaridad_lexica(self, tokens: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Analiza la polaridad usando el lexicon de opinion_lexicon.\n",
    "        \n",
    "        Args:\n",
    "            tokens (List[str]): Lista de tokens limpios\n",
    "            \n",
    "        Returns:\n",
    "            Dict: M√©tricas detalladas de polaridad\n",
    "        \"\"\"\n",
    "        # Identificar palabras por sentimiento\n",
    "        palabras_pos_encontradas = [token for token in tokens if token in self.palabras_positivas]\n",
    "        palabras_neg_encontradas = [token for token in tokens if token in self.palabras_negativas]\n",
    "        palabras_neutras = [token for token in tokens \n",
    "                           if token not in self.palabras_positivas and \n",
    "                              token not in self.palabras_negativas]\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        total_palabras = len(tokens)\n",
    "        count_positivas = len(palabras_pos_encontradas)\n",
    "        count_negativas = len(palabras_neg_encontradas)\n",
    "        count_neutras = len(palabras_neutras)\n",
    "        \n",
    "        # Calcular proporciones\n",
    "        prop_positivas = count_positivas / total_palabras if total_palabras > 0 else 0\n",
    "        prop_negativas = count_negativas / total_palabras if total_palabras > 0 else 0\n",
    "        prop_neutras = count_neutras / total_palabras if total_palabras > 0 else 0\n",
    "        \n",
    "        # Calcular puntuaci√≥n de polaridad\n",
    "        puntuacion_polaridad = (count_positivas - count_negativas) / total_palabras if total_palabras > 0 else 0\n",
    "        \n",
    "        # Frecuencias de palabras sentimentales\n",
    "        freq_positivas = Counter(palabras_pos_encontradas)\n",
    "        freq_negativas = Counter(palabras_neg_encontradas)\n",
    "        \n",
    "        return {\n",
    "            'conteos': {\n",
    "                'positivas': count_positivas,\n",
    "                'negativas': count_negativas,\n",
    "                'neutras': count_neutras,\n",
    "                'total': total_palabras\n",
    "            },\n",
    "            'proporciones': {\n",
    "                'positivas': prop_positivas,\n",
    "                'negativas': prop_negativas,\n",
    "                'neutras': prop_neutras\n",
    "            },\n",
    "            'puntuacion_polaridad': puntuacion_polaridad,\n",
    "            'palabras_positivas': palabras_pos_encontradas,\n",
    "            'palabras_negativas': palabras_neg_encontradas,\n",
    "            'freq_positivas': freq_positivas,\n",
    "            'freq_negativas': freq_negativas,\n",
    "            'top_positivas': freq_positivas.most_common(10),\n",
    "            'top_negativas': freq_negativas.most_common(10)\n",
    "        }\n",
    "    \n",
    "    def analizar_sentimiento_contextual(self, texto_original: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Analiza sentimientos considerando el contexto usando VADER.\n",
    "        \n",
    "        Args:\n",
    "            texto_original (str): Texto original sin procesar\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Puntuaciones de sentimiento contextual\n",
    "        \"\"\"\n",
    "        if self.analizador_vader is None:\n",
    "            return {'error': 'VADER no disponible'}\n",
    "        \n",
    "        # Analizar por oraciones para mejor contexto\n",
    "        oraciones = sent_tokenize(texto_original)\n",
    "        puntuaciones_oraciones: List[Any] = []\n",
    "        \n",
    "        for oracion in oraciones:\n",
    "            if len(oracion.strip()) > 10:  # Solo oraciones significativas\n",
    "                puntuacion = self.analizador_vader.polarity_scores(oracion)\n",
    "                puntuaciones_oraciones.append(puntuacion)\n",
    "        \n",
    "        # Calcular promedios\n",
    "        if puntuaciones_oraciones:\n",
    "            puntuacion_promedio = {\n",
    "                'neg': np.mean([p['neg'] for p in puntuaciones_oraciones]),\n",
    "                'neu': np.mean([p['neu'] for p in puntuaciones_oraciones]),\n",
    "                'pos': np.mean([p['pos'] for p in puntuaciones_oraciones]),\n",
    "                'compound': np.mean([p['compound'] for p in puntuaciones_oraciones])\n",
    "            }\n",
    "        else:\n",
    "            puntuacion_promedio = {'neg': 0, 'neu': 1, 'pos': 0, 'compound': 0}\n",
    "        \n",
    "        return {\n",
    "            'puntuacion_promedio': puntuacion_promedio,\n",
    "            'num_oraciones_analizadas': len(puntuaciones_oraciones),\n",
    "            'distribuci√≥n_oraciones': puntuaciones_oraciones[:10]  # Primeras 10 para muestra\n",
    "        }\n",
    "    \n",
    "    def analisis_completo_sentimientos(self, tokens_limpios: List[str], \n",
    "                                     texto_original: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Realiza un an√°lisis completo combinando m√∫ltiples enfoques.\n",
    "        \n",
    "        Args:\n",
    "            tokens_limpios (List[str]): Tokens procesados\n",
    "            texto_original (str): Texto original\n",
    "            \n",
    "        Returns:\n",
    "            Dict: An√°lisis completo de sentimientos\n",
    "        \"\"\"\n",
    "        # An√°lisis lexical\n",
    "        analisis_lexical = self.analizar_polaridad_lexica(tokens_limpios)\n",
    "        \n",
    "        # An√°lisis contextual\n",
    "        analisis_contextual = self.analizar_sentimiento_contextual(texto_original)\n",
    "        \n",
    "        # M√©tricas combinadas\n",
    "        self.metricas_sentimiento = {\n",
    "            'analisis_lexical': analisis_lexical,\n",
    "            'analisis_contextual': analisis_contextual,\n",
    "            'interpretacion': self._interpretar_resultados(analisis_lexical, analisis_contextual)\n",
    "        }\n",
    "        \n",
    "        return self.metricas_sentimiento\n",
    "    \n",
    "    def _interpretar_resultados(self, lexical: Dict, contextual: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Interpreta y combina los resultados de ambos an√°lisis.\n",
    "        \"\"\"\n",
    "        polaridad_lexical = lexical['puntuacion_polaridad']\n",
    "        \n",
    "        if 'error' not in contextual:\n",
    "            compound_score = contextual['puntuacion_promedio']['compound']\n",
    "        else:\n",
    "            compound_score = 0\n",
    "        \n",
    "        # Determinar sentimiento predominante\n",
    "        if polaridad_lexical > 0.1 or compound_score > 0.1:\n",
    "            sentimiento_general: str = \"Positivo\"\n",
    "        elif polaridad_lexical < -0.1 or compound_score < -0.1:\n",
    "            sentimiento_general: str = \"Negativo\"\n",
    "        else:\n",
    "            sentimiento_general: str = \"Neutro\"\n",
    "        \n",
    "        # Calcular intensidad\n",
    "        intensidad = abs(polaridad_lexical) + abs(compound_score) / 2\n",
    "        \n",
    "        if intensidad > 0.3:\n",
    "            nivel_intensidad: str = \"Alta\"\n",
    "        elif intensidad > 0.1:\n",
    "            nivel_intensidad: str = \"Moderada\"\n",
    "        else:\n",
    "            nivel_intensidad: str = \"Baja\"\n",
    "        \n",
    "        return {\n",
    "            'sentimiento_general': sentimiento_general,\n",
    "            'nivel_intensidad': nivel_intensidad,\n",
    "            'puntuacion_combinada': (polaridad_lexical + compound_score) / 2,\n",
    "            'confianza': min(lexical['conteos']['positivas'] + lexical['conteos']['negativas'], 100) / 100\n",
    "        }\n",
    "    \n",
    "    def mostrar_resumen_sentimientos(self):\n",
    "        \"\"\"\n",
    "        Muestra un resumen detallado del an√°lisis de sentimientos.\n",
    "        \"\"\"\n",
    "        if not self.metricas_sentimiento:\n",
    "            print(\" Debe ejecutar el an√°lisis primero\")\n",
    "            return\n",
    "        \n",
    "        lex = self.metricas_sentimiento['analisis_lexical']\n",
    "        ctx = self.metricas_sentimiento['analisis_contextual']\n",
    "        interp = self.metricas_sentimiento['interpretacion']\n",
    "        \n",
    "        print(\" AN√ÅLISIS DE SENTIMIENTOS - RESUMEN COMPLETO\")\n",
    "        print(\"=\" * 55)\n",
    "        \n",
    "        print(f\" Sentimiento General: {interp['sentimiento_general']}\")\n",
    "        print(f\" Intensidad: {interp['nivel_intensidad']}\")\n",
    "        print(f\" Puntuaci√≥n Combinada: {interp['puntuacion_combinada']:.3f}\")\n",
    "        print(f\"üîç Confianza: {interp['confianza']:.1%}\")\n",
    "        \n",
    "        print(f\"\\n AN√ÅLISIS LEXICAL:\")\n",
    "        print(f\"   Palabras positivas: {lex['conteos']['positivas']:,} ({lex['proporciones']['positivas']:.1%})\")\n",
    "        print(f\"   Palabras negativas: {lex['conteos']['negativas']:,} ({lex['proporciones']['negativas']:.1%})\")\n",
    "        print(f\"   Palabras neutras: {lex['conteos']['neutras']:,} ({lex['proporciones']['neutras']:.1%})\")\n",
    "        print(f\"   Polaridad: {lex['puntuacion_polaridad']:.3f}\")\n",
    "        \n",
    "        if 'error' not in ctx:\n",
    "            print(f\"\\nüß† AN√ÅLISIS CONTEXTUAL (VADER):\")\n",
    "            print(f\"   Negativo: {ctx['puntuacion_promedio']['neg']:.3f}\")\n",
    "            print(f\"   Neutral: {ctx['puntuacion_promedio']['neu']:.3f}\")\n",
    "            print(f\"   Positivo: {ctx['puntuacion_promedio']['pos']:.3f}\")\n",
    "            print(f\"   Compuesto: {ctx['puntuacion_promedio']['compound']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n TOP PALABRAS POSITIVAS:\")\n",
    "        for palabra, freq in lex['top_positivas'][:5]:\n",
    "            print(f\"   ‚Ä¢ {palabra}: {freq} veces\")\n",
    "        \n",
    "        print(f\"\\n TOP PALABRAS NEGATIVAS:\")\n",
    "        for palabra, freq in lex['top_negativas'][:5]:\n",
    "            print(f\"   ‚Ä¢ {palabra}: {freq} veces\")\n",
    "\n",
    "# Inicializar analizador de sentimientos\n",
    "analizador_sentimientos = AnalizadorSentimientosAvanzado()\n",
    "print(\" Analizador de sentimientos avanzado inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6929770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar an√°lisis completo de sentimientos en el texto de firefox\n",
    "tokens_firefox_limpios = resultados_limpieza_lem['normalizacion_morfologica']['tokens']\n",
    "\n",
    "print(\" Ejecutando an√°lisis completo de sentimientos...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Realizar an√°lisis completo\n",
    "resultados_sentimientos = analizador_sentimientos.analisis_completo_sentimientos(\n",
    "    tokens_firefox_limpios, \n",
    "    texto_firefox_original\n",
    ")\n",
    "\n",
    "# Mostrar resumen detallado\n",
    "analizador_sentimientos.mostrar_resumen_sentimientos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a36284",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.2 Visualizaciones Avanzadas de Sentimientos\n",
    "\n",
    "def crear_visualizaciones_sentimientos(analizador: AnalizadorSentimientosAvanzado):\n",
    "    \"\"\"\n",
    "    Crea visualizaciones comprensivas del an√°lisis de sentimientos.\n",
    "    \"\"\"\n",
    "    metricas = analizador.metricas_sentimiento\n",
    "    lex = metricas['analisis_lexical']\n",
    "    ctx = metricas['analisis_contextual']\n",
    "    \n",
    "    # Configurar figura con subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(' An√°lisis Avanzado de Sentimientos - Visualizaciones', \n",
    "                 fontsize=16, fontweight: str = \"bold\")\n",
    "    \n",
    "    # 1. Distribuci√≥n de sentimientos (Pie Chart)\n",
    "    sentimientos = ['Positivas', 'Negativas', 'Neutras']\n",
    "    conteos = [lex['conteos']['positivas'], lex['conteos']['negativas'], lex['conteos']['neutras']]\n",
    "    colores = ['lightgreen', 'lightcoral', 'lightgray']\n",
    "    \n",
    "    wedges, texts, autotexts = axes[0,0].pie(conteos, labels=sentimientos, colors=colores, \n",
    "                                           autopct: str = \"%1.1f%%\", startangle=90)\n",
    "    axes[0,0].set_title(' Distribuci√≥n de Sentimientos', fontweight: str = \"bold\")\n",
    "    \n",
    "    # 2. Top palabras positivas vs negativas\n",
    "    if lex['top_positivas'] and lex['top_negativas']:\n",
    "        # Preparar datos para comparaci√≥n\n",
    "        top_pos = lex['top_positivas'][:8]\n",
    "        top_neg = lex['top_negativas'][:8]\n",
    "        \n",
    "        palabras_pos = [palabra for palabra, _ in top_pos]\n",
    "        freq_pos = [freq for _, freq in top_pos]\n",
    "        palabras_neg = [palabra for palabra, _ in top_neg]\n",
    "        freq_neg = [freq for _, freq in top_neg]\n",
    "        \n",
    "        y_pos = np.arange(len(palabras_pos))\n",
    "        y_neg = np.arange(len(palabras_neg))\n",
    "        \n",
    "        axes[0,1].barh(y_pos, freq_pos, color: str = \"lightgreen\", alpha=0.7, label: str = \"Positivas\")\n",
    "        axes[0,1].barh(y_neg - 0.4, [-f for f in freq_neg], color: str = \"lightcoral\", alpha=0.7, label: str = \"Negativas\")\n",
    "        \n",
    "        axes[0,1].set_yticks(list(y_pos) + [y - 0.4 for y in y_neg])\n",
    "        axes[0,1].set_yticklabels(palabras_pos + palabras_neg)\n",
    "        axes[0,1].set_title(' Top Palabras por Sentimiento', fontweight: str = \"bold\")\n",
    "        axes[0,1].set_xlabel('Frecuencia')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].axvline(x=0, color: str = \"black\", linestyle: str = \"-\", alpha=0.3)\n",
    "    \n",
    "    # 3. An√°lisis VADER (si est√° disponible)\n",
    "    if 'error' not in ctx:\n",
    "        vader_labels = ['Negativo', 'Neutral', 'Positivo']\n",
    "        vader_scores = [\n",
    "            ctx['puntuacion_promedio']['neg'],\n",
    "            ctx['puntuacion_promedio']['neu'],\n",
    "            ctx['puntuacion_promedio']['pos']\n",
    "        ]\n",
    "        colors_vader = ['red', 'gray', 'green']\n",
    "        \n",
    "        bars = axes[0,2].bar(vader_labels, vader_scores, color=colors_vader, alpha=0.7)\n",
    "        axes[0,2].set_title('üß† An√°lisis VADER', fontweight: str = \"bold\")\n",
    "        axes[0,2].set_ylabel('Puntuaci√≥n')\n",
    "        axes[0,2].set_ylim(0, 1)\n",
    "        \n",
    "        # Agregar valores en las barras\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[0,2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                          f'{height:.3f}', ha: str = \"center\", va: str = \"bottom\")\n",
    "    else:\n",
    "        axes[0,2].text(0.5, 0.5, 'VADER no disponible', ha: str = \"center\", va: str = \"center\", \n",
    "                      transform=axes[0,2].transAxes, fontsize=12)\n",
    "        axes[0,2].set_title('üß† An√°lisis VADER', fontweight: str = \"bold\")\n",
    "    \n",
    "    # 4. Comparaci√≥n de m√©todos (Radar Chart simulado con barras)\n",
    "    if 'error' not in ctx:\n",
    "        metodos = ['Lexical\\n(Positivo)', 'Lexical\\n(Negativo)', 'VADER\\n(Compound)']\n",
    "        valores = [\n",
    "            lex['proporciones']['positivas'],\n",
    "            lex['proporciones']['negativas'],\n",
    "            abs(ctx['puntuacion_promedio']['compound'])\n",
    "        ]\n",
    "        \n",
    "        bars = axes[1,0].bar(metodos, valores, color=['lightgreen', 'lightcoral', 'skyblue'], alpha=0.7)\n",
    "        axes[1,0].set_title(' Comparaci√≥n de M√©todos', fontweight: str = \"bold\")\n",
    "        axes[1,0].set_ylabel('Intensidad')\n",
    "        \n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[1,0].text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                          f'{height:.3f}', ha: str = \"center\", va: str = \"bottom\")\n",
    "    \n",
    "    # 5. Evoluci√≥n del sentimiento a lo largo del texto\n",
    "    # Dividir tokens en segmentos y analizar sentimiento por segmento\n",
    "    tokens = lex['palabras_positivas'] + lex['palabras_negativas']\n",
    "    segmento_tama√±o = max(1, len(tokens_firefox_limpios) // 10)\n",
    "    \n",
    "    polaridades_segmentos: List[Any] = []\n",
    "    for i in range(0, len(tokens_firefox_limpios), segmento_tama√±o):\n",
    "        segmento = tokens_firefox_limpios[i:i+segmento_tama√±o]\n",
    "        pos_seg = len([t for t in segmento if t in analizador.palabras_positivas])\n",
    "        neg_seg = len([t for t in segmento if t in analizador.palabras_negativas])\n",
    "        \n",
    "        if len(segmento) > 0:\n",
    "            polaridad_seg = (pos_seg - neg_seg) / len(segmento)\n",
    "            polaridades_segmentos.append(polaridad_seg)\n",
    "    \n",
    "    if polaridades_segmentos:\n",
    "        axes[1,1].plot(range(1, len(polaridades_segmentos)+1), polaridades_segmentos, \n",
    "                      marker: str = \"o\", linewidth=2, markersize=6, color: str = \"purple\")\n",
    "        axes[1,1].axhline(y=0, color: str = \"black\", linestyle: str = \"--\", alpha=0.5)\n",
    "        axes[1,1].set_title(' Evoluci√≥n del Sentimiento', fontweight: str = \"bold\")\n",
    "        axes[1,1].set_xlabel('Segmento del Texto')\n",
    "        axes[1,1].set_ylabel('Polaridad')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. M√©tricas resumen\n",
    "    axes[1,2].axis('off')\n",
    "    resumen_texto = f\"\"\"\n",
    "     M√âTRICAS RESUMEN\n",
    "    \n",
    "     Sentimiento: {metricas['interpretacion']['sentimiento_general']}\n",
    "     Intensidad: {metricas['interpretacion']['nivel_intensidad']}\n",
    "    \n",
    "     LEXICAL\n",
    "    Positivas: {lex['conteos']['positivas']} ({lex['proporciones']['positivas']:.1%})\n",
    "    Negativas: {lex['conteos']['negativas']} ({lex['proporciones']['negativas']:.1%})\n",
    "    Polaridad: {lex['puntuacion_polaridad']:.3f}\n",
    "    \n",
    "    üß† CONTEXTUAL (VADER)\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'error' not in ctx:\n",
    "        resumen_texto += f\"\"\"Compound: {ctx['puntuacion_promedio']['compound']:.3f}\n",
    "    Confianza: {metricas['interpretacion']['confianza']:.1%}\"\"\"\n",
    "    else:\n",
    "        resumen_texto += \"No disponible\"\n",
    "    \n",
    "    axes[1,2].text(0.1, 0.9, resumen_texto, transform=axes[1,2].transAxes, \n",
    "                   fontsize=10, verticalalignment: str = \"top\", fontfamily: str = \"monospace\",\n",
    "                   bbox=dict(boxstyle: str = \"round,pad=0.5\", facecolor: str = \"lightyellow\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generar visualizaciones de sentimientos\n",
    "crear_visualizaciones_sentimientos(analizador_sentimientos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b07737",
   "metadata": {},
   "source": [
    "<a id=\"wordnet\"></a>\n",
    "<div style=\"background:#8E44AD;padding:20px;color:#ffffff;margin-top:10px;border-radius:8px;\">\n",
    "\n",
    "#  6. An√°lisis Sem√°ntico con WordNet\n",
    "\n",
    "**Objetivo:** Desarrollar una funci√≥n que extraiga synsets de palabras seg√∫n su funci√≥n gramatical usando WordNet.\n",
    "\n",
    "**Puntuaci√≥n m√°xima:** 4 puntos\n",
    "\n",
    "</div>\n",
    "\n",
    "### 6.1 Sistema Avanzado de An√°lisis Sem√°ntico\n",
    "\n",
    "WordNet es una base de datos l√©xica que organiza palabras en conjuntos de sin√≥nimos llamados **synsets** (conjuntos de sin√≥nimos). Cada synset representa un concepto distinto y proporciona definiciones, ejemplos y relaciones sem√°nticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec9acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "class AnalizadorSemanticoWordNet:\n",
    "    \"\"\"\n",
    "    Clase especializada para an√°lisis sem√°ntico avanzado usando WordNet.\n",
    "    Extrae synsets basados en la funci√≥n gramatical de las palabras.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Mapeo de etiquetas POS de Penn Treebank a WordNet\n",
    "        self.mapeo_pos_wordnet = {\n",
    "            'J': wn.ADJ,      # Adjetivos\n",
    "            'V': wn.VERB,     # Verbos\n",
    "            'N': wn.NOUN,     # Sustantivos\n",
    "            'R': wn.ADV       # Adverbios\n",
    "        }\n",
    "        \n",
    "        # Estad√≠sticas del an√°lisis\n",
    "        self.estadisticas_synsets: Dict[str, Any] = {}\n",
    "        \n",
    "    def convertir_pos_wordnet(self, etiqueta_treebank: str) -> str:\n",
    "        \"\"\"\n",
    "        Convierte etiquetas POS de Treebank a formato WordNet.\n",
    "        \n",
    "        Args:\n",
    "            etiqueta_treebank (str): Etiqueta POS de Penn Treebank\n",
    "            \n",
    "        Returns:\n",
    "            str: Etiqueta POS de WordNet o None si no es relevante\n",
    "        \"\"\"\n",
    "        # Obtener primera letra de la etiqueta\n",
    "        primera_letra = etiqueta_treebank[0].upper()\n",
    "        return self.mapeo_pos_wordnet.get(primera_letra, None)\n",
    "    \n",
    "    def extraer_synsets_por_categoria(self, textos: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Extrae synsets organizados por categor√≠a gramatical.\n",
    "        \n",
    "        Args:\n",
    "            textos (List[str]): Lista de textos a analizar\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Synsets organizados por palabra y categor√≠a gramatical\n",
    "        \"\"\"\n",
    "        # Estructura para almacenar resultados\n",
    "        synsets_por_categoria = {\n",
    "            'sustantivos': defaultdict(list),\n",
    "            'verbos': defaultdict(list),\n",
    "            'adjetivos': defaultdict(list),\n",
    "            'adverbios': defaultdict(list),\n",
    "            'estadisticas': {\n",
    "                'palabras_procesadas': 0,\n",
    "                'synsets_encontrados': 0,\n",
    "                'palabras_con_synsets': 0,\n",
    "                'distribucion_pos': {'N': 0, 'V': 0, 'J': 0, 'R': 0}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Mapeo de categor√≠as para organizaci√≥n\n",
    "        mapeo_categorias = {\n",
    "            wn.NOUN: 'sustantivos',\n",
    "            wn.VERB: 'verbos', \n",
    "            wn.ADJ: 'adjetivos',\n",
    "            wn.ADV: 'adverbios'\n",
    "        }\n",
    "        \n",
    "        print(\"üîç Extrayendo synsets por categor√≠a gramatical...\")\n",
    "        \n",
    "        for texto in textos:\n",
    "            try:\n",
    "                # Tokenizar y etiquetar POS\n",
    "                tokens = word_tokenize(texto.lower())\n",
    "                etiquetas_pos = pos_tag(tokens)\n",
    "                \n",
    "                for palabra, etiqueta_pos in etiquetas_pos:\n",
    "                    # Solo procesar palabras alfab√©ticas de m√°s de 2 caracteres\n",
    "                    if palabra.isalpha() and len(palabra) > 2:\n",
    "                        synsets_por_categoria['estadisticas']['palabras_procesadas'] += 1\n",
    "                        \n",
    "                        # Convertir etiqueta POS a formato WordNet\n",
    "                        pos_wordnet = self.convertir_pos_wordnet(etiqueta_pos)\n",
    "                        \n",
    "                        if pos_wordnet:\n",
    "                            # Buscar synsets para la palabra con la categor√≠a espec√≠fica\n",
    "                            synsets = wn.synsets(palabra, pos=pos_wordnet)\n",
    "                            \n",
    "                            if synsets:\n",
    "                                synsets_por_categoria['estadisticas']['palabras_con_synsets'] += 1\n",
    "                                categoria = mapeo_categorias[pos_wordnet]\n",
    "                                \n",
    "                                # Almacenar informaci√≥n de synsets\n",
    "                                for synset in synsets:\n",
    "                                    info_synset = {\n",
    "                                        'nombre': synset.name(),\n",
    "                                        'definicion': synset.definition(),\n",
    "                                        'ejemplos': synset.examples(),\n",
    "                                        'lemas': [lema.name() for lema in synset.lemmas()],\n",
    "                                        'categoria_pos': pos_wordnet\n",
    "                                    }\n",
    "                                    \n",
    "                                    # Evitar duplicados\n",
    "                                    if info_synset not in synsets_por_categoria[categoria][palabra]:\n",
    "                                        synsets_por_categoria[categoria][palabra].append(info_synset)\n",
    "                                        synsets_por_categoria['estadisticas']['synsets_encontrados'] += 1\n",
    "                                \n",
    "                                # Actualizar estad√≠sticas de distribuci√≥n POS\n",
    "                                pos_key = etiqueta_pos[0].upper()\n",
    "                                if pos_key in synsets_por_categoria['estadisticas']['distribucion_pos']:\n",
    "                                    synsets_por_categoria['estadisticas']['distribucion_pos'][pos_key] += 1\n",
    "                                    \n",
    "            except Exception as e:\n",
    "                print(f\" Error procesando texto: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        # Convertir defaultdict a dict regular para serializaci√≥n\n",
    "        for categoria in ['sustantivos', 'verbos', 'adjetivos', 'adverbios']:\n",
    "            synsets_por_categoria[categoria] = dict(synsets_por_categoria[categoria])\n",
    "        \n",
    "        self.estadisticas_synsets = synsets_por_categoria['estadisticas']\n",
    "        return synsets_por_categoria\n",
    "    \n",
    "    def analizar_riqueza_semantica(self, synsets_data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Analiza la riqueza sem√°ntica del corpus basada en los synsets.\n",
    "        \n",
    "        Args:\n",
    "            synsets_data (Dict): Datos de synsets extra√≠dos\n",
    "            \n",
    "        Returns:\n",
    "            Dict: M√©tricas de riqueza sem√°ntica\n",
    "        \"\"\"\n",
    "        stats = synsets_data['estadisticas']\n",
    "        \n",
    "        # Calcular m√©tricas de riqueza sem√°ntica\n",
    "        cobertura_semantica = (stats['palabras_con_synsets'] / \n",
    "                              max(stats['palabras_procesadas'], 1))\n",
    "        \n",
    "        promedio_synsets_palabra = (stats['synsets_encontrados'] / \n",
    "                                   max(stats['palabras_con_synsets'], 1))\n",
    "        \n",
    "        # Analizar distribuci√≥n por categor√≠as\n",
    "        total_categorias = sum(stats['distribucion_pos'].values())\n",
    "        distribucion_normalizada = {\n",
    "            pos: count / max(total_categorias, 1) \n",
    "            for pos, count in stats['distribucion_pos'].items()\n",
    "        }\n",
    "        \n",
    "        # Encontrar categor√≠a dominante\n",
    "        categoria_dominante = max(stats['distribucion_pos'].items(), key=lambda x: x[1])\n",
    "        \n",
    "        # Calcular diversidad sem√°ntica (usando √≠ndice de Shannon)\n",
    "        from math import log\n",
    "        diversidad_shannon = 0\n",
    "        for proporcion in distribucion_normalizada.values():\n",
    "            if proporcion > 0:\n",
    "                diversidad_shannon -= proporcion * log(proporcion)\n",
    "        \n",
    "        return {\n",
    "            'cobertura_semantica': cobertura_semantica,\n",
    "            'promedio_synsets_por_palabra': promedio_synsets_palabra,\n",
    "            'distribucion_pos_normalizada': distribucion_normalizada,\n",
    "            'categoria_dominante': categoria_dominante,\n",
    "            'diversidad_shannon': diversidad_shannon,\n",
    "            'interpretacion_diversidad': self._interpretar_diversidad(diversidad_shannon)\n",
    "        }\n",
    "    \n",
    "    def _interpretar_diversidad(self, shannon_index: float) -> str:\n",
    "        \"\"\"Interpreta el √≠ndice de diversidad de Shannon.\"\"\"\n",
    "        if shannon_index > 1.2:\n",
    "            return \"Alta diversidad gramatical\"\n",
    "        elif shannon_index > 0.8:\n",
    "            return \"Diversidad moderada\"\n",
    "        else:\n",
    "            return \"Baja diversidad gramatical\"\n",
    "    \n",
    "    def mostrar_resumen_semantico(self, synsets_data: Dict, riqueza_data: Dict):\n",
    "        \"\"\"\n",
    "        Muestra un resumen completo del an√°lisis sem√°ntico.\n",
    "        \"\"\"\n",
    "        stats = synsets_data['estadisticas']\n",
    "        \n",
    "        print(\" AN√ÅLISIS SEM√ÅNTICO - RESUMEN COMPLETO\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        print(f\" ESTAD√çSTICAS GENERALES:\")\n",
    "        print(f\"   Palabras procesadas: {stats['palabras_procesadas']:,}\")\n",
    "        print(f\"   Palabras con synsets: {stats['palabras_con_synsets']:,}\")\n",
    "        print(f\"   Total synsets encontrados: {stats['synsets_encontrados']:,}\")\n",
    "        print(f\"   Cobertura sem√°ntica: {riqueza_data['cobertura_semantica']:.1%}\")\n",
    "        print(f\"   Promedio synsets/palabra: {riqueza_data['promedio_synsets_por_palabra']:.2f}\")\n",
    "        \n",
    "        print(f\"\\n DISTRIBUCI√ìN POR CATEGOR√çAS:\")\n",
    "        categorias_nombres = {'N': 'Sustantivos', 'V': 'Verbos', 'J': 'Adjetivos', 'R': 'Adverbios'}\n",
    "        for pos, count in stats['distribucion_pos'].items():\n",
    "            proporcion = riqueza_data['distribucion_pos_normalizada'][pos]\n",
    "            nombre_categoria = categorias_nombres.get(pos, pos)\n",
    "            print(f\"   {nombre_categoria}: {count:,} ({proporcion:.1%})\")\n",
    "        \n",
    "        print(f\"\\n RIQUEZA SEM√ÅNTICA:\")\n",
    "        print(f\"   Categor√≠a dominante: {categorias_nombres.get(riqueza_data['categoria_dominante'][0], 'N/A')} \"\n",
    "              f\"({riqueza_data['categoria_dominante'][1]:,} palabras)\")\n",
    "        print(f\"   Diversidad Shannon: {riqueza_data['diversidad_shannon']:.3f}\")\n",
    "        print(f\"   Interpretaci√≥n: {riqueza_data['interpretacion_diversidad']}\")\n",
    "        \n",
    "        # Mostrar ejemplos de cada categor√≠a\n",
    "        print(f\"\\nüîç EJEMPLOS POR CATEGOR√çA:\")\n",
    "        categorias_mostrar = ['sustantivos', 'verbos', 'adjetivos', 'adverbios']\n",
    "        \n",
    "        for categoria in categorias_mostrar:\n",
    "            palabras_categoria = synsets_data[categoria]\n",
    "            if palabras_categoria:\n",
    "                print(f\"\\n    {categoria.upper()}:\")\n",
    "                # Mostrar hasta 3 ejemplos\n",
    "                for i, (palabra, synsets_info) in enumerate(list(palabras_categoria.items())[:3]):\n",
    "                    print(f\"      ‚Ä¢ {palabra}:\")\n",
    "                    if synsets_info:\n",
    "                        primer_synset = synsets_info[0]\n",
    "                        print(f\"        - Definici√≥n: {primer_synset['definicion'][:80]}...\")\n",
    "                        if primer_synset['ejemplos']:\n",
    "                            print(f\"        - Ejemplo: {primer_synset['ejemplos'][0][:60]}...\")\n",
    "    \n",
    "    def obtener_synsets_palabra_especifica(self, palabra: str, pos_especifica: str = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Obtiene informaci√≥n detallada de synsets para una palabra espec√≠fica.\n",
    "        \n",
    "        Args:\n",
    "            palabra (str): Palabra a analizar\n",
    "            pos_especifica (str): Categor√≠a POS espec√≠fica (opcional)\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Lista de synsets con informaci√≥n detallada\n",
    "        \"\"\"\n",
    "        synsets_info: List[Any] = []\n",
    "        \n",
    "        # Buscar synsets\n",
    "        if pos_especifica:\n",
    "            synsets = wn.synsets(palabra, pos=pos_especifica)\n",
    "        else:\n",
    "            synsets = wn.synsets(palabra)\n",
    "        \n",
    "        for synset in synsets:\n",
    "            info = {\n",
    "                'nombre': synset.name(),\n",
    "                'definicion': synset.definition(),\n",
    "                'ejemplos': synset.examples(),\n",
    "                'categoria': synset.pos(),\n",
    "                'hiper√≥nimos': [h.name() for h in synset.hypernyms()],\n",
    "                'hip√≥nimos': [h.name() for h in synset.hyponyms()[:5]],  # Limitamos a 5\n",
    "                'sin√≥nimos': [lema.name() for lema in synset.lemmas()],\n",
    "                'ant√≥nimos': []\n",
    "            }\n",
    "            \n",
    "            # Buscar ant√≥nimos\n",
    "            for lema in synset.lemmas():\n",
    "                if lema.antonyms():\n",
    "                    info['ant√≥nimos'].extend([ant.name() for ant in lema.antonyms()])\n",
    "            \n",
    "            synsets_info.append(info)\n",
    "        \n",
    "        return synsets_info\n",
    "\n",
    "# Inicializar analizador sem√°ntico\n",
    "analizador_semantico = AnalizadorSemanticoWordNet()\n",
    "print(\" Analizador sem√°ntico WordNet inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar an√°lisis sem√°ntico al corpus firefox.txt\n",
    "print(\" Ejecutando an√°lisis sem√°ntico completo con WordNet...\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Dividir el texto en segmentos para procesamiento eficiente\n",
    "lineas_texto = texto_firefox_original.split('\\n')\n",
    "# Filtrar l√≠neas significativas (m√°s de 10 caracteres)\n",
    "textos_filtrados = [linea.strip() for linea in lineas_texto \n",
    "                   if linea.strip() and len(linea.strip()) > 10]\n",
    "\n",
    "# Tomar una muestra representativa para el an√°lisis (primeros 50 segmentos)\n",
    "muestra_textos = textos_filtrados[:50]\n",
    "\n",
    "print(f\" Procesando {len(muestra_textos)} segmentos de texto...\")\n",
    "\n",
    "# Extraer synsets por categor√≠a\n",
    "resultados_synsets = analizador_semantico.extraer_synsets_por_categoria(muestra_textos)\n",
    "\n",
    "# Analizar riqueza sem√°ntica\n",
    "analisis_riqueza = analizador_semantico.analizar_riqueza_semantica(resultados_synsets)\n",
    "\n",
    "# Mostrar resumen completo\n",
    "analizador_semantico.mostrar_resumen_semantico(resultados_synsets, analisis_riqueza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6.2 Demostraci√≥n de Funci√≥n Espec√≠fica\n",
    "\n",
    "def extraer_synsets_desde_textos(textos: List[str]) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Funci√≥n principal requerida: Extrae synsets de palabras seg√∫n su funci√≥n gramatical.\n",
    "    \n",
    "    Args:\n",
    "        textos (List[str]): Conjunto de textos a analizar\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, List[Dict]]: Estructura con synsets organizados por palabra y funci√≥n gramatical\n",
    "    \"\"\"\n",
    "    resultado_final: Dict[str, Any] = {}\n",
    "    analizador = AnalizadorSemanticoWordNet()\n",
    "    \n",
    "    # Procesar textos\n",
    "    synsets_data = analizador.extraer_synsets_por_categoria(textos)\n",
    "    \n",
    "    # Reorganizar en el formato solicitado\n",
    "    for categoria in ['sustantivos', 'verbos', 'adjetivos', 'adverbios']:\n",
    "        for palabra, synsets_info in synsets_data[categoria].items():\n",
    "            if palabra not in resultado_final:\n",
    "                resultado_final[palabra] = []\n",
    "            \n",
    "            for synset_info in synsets_info:\n",
    "                entrada_synset = {\n",
    "                    'synset_nombre': synset_info['nombre'],\n",
    "                    'definicion': synset_info['definicion'],\n",
    "                    'categoria_gramatical': synset_info['categoria_pos'],\n",
    "                    'ejemplos': synset_info['ejemplos'],\n",
    "                    'sinonimos': synset_info['lemas']\n",
    "                }\n",
    "                resultado_final[palabra].append(entrada_synset)\n",
    "    \n",
    "    return resultado_final\n",
    "\n",
    "# Probar la funci√≥n con textos de ejemplo\n",
    "textos_prueba = [\n",
    "    \"The browser works very well and users love the new features.\",\n",
    "    \"Firefox provides excellent security and fast browsing experience.\",\n",
    "    \"Many people appreciate good software design and functionality.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ PROBANDO FUNCI√ìN PRINCIPAL:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Textos de prueba:\")\n",
    "for i, texto in enumerate(textos_prueba, 1):\n",
    "    print(f\"{i}. {texto}\")\n",
    "\n",
    "print(f\"\\nüîç Ejecutando funci√≥n extraer_synsets_desde_textos()...\")\n",
    "resultado_funcion = extraer_synsets_desde_textos(textos_prueba)\n",
    "\n",
    "print(f\"\\n RESULTADOS:\")\n",
    "print(f\"Palabras analizadas: {len(resultado_funcion):,}\")\n",
    "\n",
    "# Mostrar ejemplos de resultados\n",
    "print(f\"\\n EJEMPLOS DE SYNSETS EXTRA√çDOS:\")\n",
    "ejemplos_mostrados = 0\n",
    "for palabra, synsets in resultado_funcion.items():\n",
    "    if ejemplos_mostrados >= 5:  # Mostrar solo 5 ejemplos\n",
    "        break\n",
    "    \n",
    "    print(f\"\\n Palabra: '{palabra}'\")\n",
    "    for i, synset in enumerate(synsets[:2]):  # M√°ximo 2 synsets por palabra\n",
    "        print(f\"   Synset {i+1}: {synset['synset_nombre']}\")\n",
    "        print(f\"   Categor√≠a: {synset['categoria_gramatical']}\")\n",
    "        print(f\"   Definici√≥n: {synset['definicion'][:80]}...\")\n",
    "        if synset['ejemplos']:\n",
    "            print(f\"   Ejemplo: {synset['ejemplos'][0][:60]}...\")\n",
    "        print(f\"   Sin√≥nimos: {', '.join(synset['sinonimos'][:3])}...\")\n",
    "    \n",
    "    ejemplos_mostrados += 1\n",
    "\n",
    "print(f\"\\n Funci√≥n probada exitosamente con {len(resultado_funcion)} palabras analizadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3015a",
   "metadata": {},
   "source": [
    "<a id=\"visualizacion\"></a>\n",
    "<div style=\"background:#16A085;padding:20px;color:#ffffff;margin-top:10px;border-radius:8px;\">\n",
    "\n",
    "#  7. Visualizaci√≥n Integral de Resultados\n",
    "\n",
    "**Objetivo:** Crear visualizaciones comprehensivas que muestren todos los resultados obtenidos durante el an√°lisis.\n",
    "\n",
    "</div>\n",
    "\n",
    "### 7.1 Dashboard Final de Resultados\n",
    "\n",
    "Presentaremos un resumen visual que integra todos los an√°lisis realizados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093401c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_dashboard_final_resultados():\n",
    "    \"\"\"\n",
    "    Crea un dashboard comprehensivo con todos los resultados del an√°lisis.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "    fig.suptitle(' Dashboard Final - An√°lisis Completo del Corpus Firefox.txt', \n",
    "                 fontsize=18, fontweight: str = \"bold\", y=0.98)\n",
    "    \n",
    "    # 1. M√©tricas generales del corpus\n",
    "    axes[0,0].axis('off')\n",
    "    metricas_texto = f\"\"\"\n",
    "     M√âTRICAS GENERALES\n",
    "    \n",
    "    Palabras originales: {estadisticas_completas['total_palabras']:,}\n",
    "    Palabras √∫nicas: {estadisticas_completas['palabras_unicas']:,}\n",
    "    Riqueza l√©xica: {estadisticas_completas['riqueza_lexica']:.3f}\n",
    "    \n",
    "    Despu√©s de limpieza:\n",
    "    Tokens finales: {len(tokens_firefox_limpios):,}\n",
    "    Tokens √∫nicos: {len(set(tokens_firefox_limpios)):,}\n",
    "    \n",
    "    Reducci√≥n vocabulario: {(1 - len(set(tokens_firefox_limpios))/estadisticas_completas['palabras_unicas']):.1%}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[0,0].text(0.05, 0.95, metricas_texto, transform=axes[0,0].transAxes, \n",
    "                   fontsize=10, verticalalignment: str = \"top\", fontfamily: str = \"monospace\",\n",
    "                   bbox=dict(boxstyle: str = \"round,pad=0.5\", facecolor: str = \"lightblue\", alpha=0.8))\n",
    "    axes[0,0].set_title(' Resumen General', fontweight: str = \"bold\")\n",
    "    \n",
    "    # 2. Distribuci√≥n de longitudes (antes vs despu√©s)\n",
    "    longitudes_original = [len(palabra) for palabra in analizador_exploratorio.tokens_filtrados]\n",
    "    longitudes_limpio = [len(token) for token in tokens_firefox_limpios]\n",
    "    \n",
    "    axes[0,1].hist(longitudes_original, bins=range(1, 16), alpha=0.6, label: str = \"Original\", color: str = \"lightcoral\")\n",
    "    axes[0,1].hist(longitudes_limpio, bins=range(1, 16), alpha=0.6, label: str = \"Limpio\", color: str = \"lightgreen\")\n",
    "    axes[0,1].set_title(' Distribuci√≥n Longitudes', fontweight: str = \"bold\")\n",
    "    axes[0,1].set_xlabel('Longitud')\n",
    "    axes[0,1].set_ylabel('Frecuencia')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # 3. Top palabras finales vs originales\n",
    "    freq_original = Counter(analizador_exploratorio.tokens_filtrados)\n",
    "    freq_limpio = Counter(tokens_firefox_limpios)\n",
    "    \n",
    "    top_original = [palabra for palabra, _ in freq_original.most_common(10)]\n",
    "    top_limpio = [palabra for palabra, _ in freq_limpio.most_common(10)]\n",
    "    \n",
    "    # Mostrar solo palabras limpias\n",
    "    freq_values = [freq for _, freq in freq_limpio.most_common(10)]\n",
    "    y_pos = np.arange(len(top_limpio))\n",
    "    \n",
    "    bars = axes[0,2].barh(y_pos, freq_values, color: str = \"lightgreen\", alpha=0.8)\n",
    "    axes[0,2].set_yticks(y_pos)\n",
    "    axes[0,2].set_yticklabels(top_limpio)\n",
    "    axes[0,2].set_title(' Top 10 Palabras Finales', fontweight: str = \"bold\")\n",
    "    axes[0,2].set_xlabel('Frecuencia')\n",
    "    \n",
    "    # 4. An√°lisis de sentimientos - Resumen\n",
    "    if 'metricas_sentimiento' in globals() and analizador_sentimientos.metricas_sentimiento:\n",
    "        lex = analizador_sentimientos.metricas_sentimiento['analisis_lexical']\n",
    "        sentimientos = ['Positivas', 'Negativas', 'Neutras']\n",
    "        conteos_sent = [lex['conteos']['positivas'], lex['conteos']['negativas'], lex['conteos']['neutras']]\n",
    "        colores_sent = ['lightgreen', 'lightcoral', 'lightgray']\n",
    "        \n",
    "        axes[0,3].pie(conteos_sent, labels=sentimientos, colors=colores_sent, autopct: str = \"%1.1f%%\")\n",
    "        axes[0,3].set_title(' Distribuci√≥n Sentimientos', fontweight: str = \"bold\")\n",
    "    else:\n",
    "        axes[0,3].text(0.5, 0.5, 'An√°lisis de sentimientos\\nno disponible', \n",
    "                      ha: str = \"center\", va: str = \"center\", transform=axes[0,3].transAxes)\n",
    "        axes[0,3].set_title(' Sentimientos', fontweight: str = \"bold\")\n",
    "    \n",
    "    # 5. Proceso de limpieza - Reducci√≥n por etapa\n",
    "    if 'resultados_limpieza_lem' in globals():\n",
    "        etapas_limpieza = ['Original', 'B√°sica', 'Intermedia', 'Filtrado', 'Lemmatizado']\n",
    "        conteos_etapas = [\n",
    "            resultados_limpieza_lem['original']['palabras'],\n",
    "            resultados_limpieza_lem['limpieza_basica']['palabras'],\n",
    "            resultados_limpieza_lem['limpieza_intermedia']['palabras'],\n",
    "            resultados_limpieza_lem['tokenizacion_filtrado']['cantidad_tokens'],\n",
    "            resultados_limpieza_lem['normalizacion_morfologica']['cantidad_tokens']\n",
    "        ]\n",
    "        \n",
    "        axes[1,0].plot(etapas_limpieza, conteos_etapas, marker: str = \"o\", linewidth=3, markersize=8, color: str = \"red\")\n",
    "        axes[1,0].set_title('üßπ Proceso de Limpieza', fontweight: str = \"bold\")\n",
    "        axes[1,0].set_ylabel('N√∫mero de Tokens')\n",
    "        axes[1,0].tick_params(axis: str = \"x\", rotation=45)\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. WordNet - Distribuci√≥n por categor√≠as gramaticales\n",
    "    if 'resultados_synsets' in globals():\n",
    "        stats_wn = resultados_synsets['estadisticas']\n",
    "        categorias_wn = ['Sustantivos', 'Verbos', 'Adjetivos', 'Adverbios']\n",
    "        conteos_wn = [stats_wn['distribucion_pos'].get(k, 0) for k in ['N', 'V', 'J', 'R']]\n",
    "        \n",
    "        bars_wn = axes[1,1].bar(categorias_wn, conteos_wn, color=['skyblue', 'lightgreen', 'orange', 'pink'], alpha=0.8)\n",
    "        axes[1,1].set_title(' WordNet - Categor√≠as POS', fontweight: str = \"bold\")\n",
    "        axes[1,1].set_ylabel('Cantidad')\n",
    "        axes[1,1].tick_params(axis: str = \"x\", rotation=45)\n",
    "        \n",
    "        # Agregar valores en barras\n",
    "        for bar in bars_wn:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                              f'{int(height)}', ha: str = \"center\", va: str = \"bottom\")\n",
    "    \n",
    "    # 7. Comparaci√≥n Lemmatizaci√≥n vs Stemming\n",
    "    if 'resultados_limpieza_stem' in globals():\n",
    "        metodos_norm = ['Lemmatizaci√≥n', 'Stemming']\n",
    "        tokens_unicos = [\n",
    "            resultados_limpieza_lem['normalizacion_morfologica']['tokens_unicos'],\n",
    "            resultados_limpieza_stem['normalizacion_morfologica']['tokens_unicos']\n",
    "        ]\n",
    "        \n",
    "        bars_norm = axes[1,2].bar(metodos_norm, tokens_unicos, color=['lightblue', 'lightcoral'], alpha=0.8)\n",
    "        axes[1,2].set_title(' Normalizaci√≥n: Lem vs Stem', fontweight: str = \"bold\")\n",
    "        axes[1,2].set_ylabel('Tokens √önicos')\n",
    "        \n",
    "        for bar in bars_norm:\n",
    "            height = bar.get_height()\n",
    "            axes[1,2].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                          f'{int(height)}', ha: str = \"center\", va: str = \"bottom\", fontweight: str = \"bold\")\n",
    "    \n",
    "    # 8. M√©tricas de calidad del an√°lisis\n",
    "    axes[1,3].axis('off')\n",
    "    if 'analisis_riqueza' in globals():\n",
    "        calidad_texto = f\"\"\"\n",
    "         CALIDAD DEL AN√ÅLISIS\n",
    "        \n",
    "        Cobertura sem√°ntica: {analisis_riqueza['cobertura_semantica']:.1%}\n",
    "        \n",
    "        Synsets por palabra: {analisis_riqueza['promedio_synsets_por_palabra']:.2f}\n",
    "        \n",
    "        Diversidad Shannon: {analisis_riqueza['diversidad_shannon']:.3f}\n",
    "        \n",
    "        {analisis_riqueza['interpretacion_diversidad']}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        calidad_texto: str = \"\"\"\n",
    "         CALIDAD DEL AN√ÅLISIS\n",
    "        \n",
    "        An√°lisis sem√°ntico\n",
    "        no completado\n",
    "        \"\"\"\n",
    "    \n",
    "    axes[1,3].text(0.05, 0.95, calidad_texto, transform=axes[1,3].transAxes, \n",
    "                   fontsize=10, verticalalignment: str = \"top\", fontfamily: str = \"monospace\",\n",
    "                   bbox=dict(boxstyle: str = \"round,pad=0.5\", facecolor: str = \"lightyellow\", alpha=0.8))\n",
    "    \n",
    "    # 9-12. Gr√°ficos adicionales de tendencias y comparaciones\n",
    "    # Evoluci√≥n de la riqueza l√©xica\n",
    "    if len(tokens_firefox_limpios) > 100:\n",
    "        segmento_tama√±o = len(tokens_firefox_limpios) // 8\n",
    "        riquezas_evolucion: List[Any] = []\n",
    "        \n",
    "        for i in range(0, len(tokens_firefox_limpios), segmento_tama√±o):\n",
    "            segmento = tokens_firefox_limpios[i:i+segmento_tama√±o]\n",
    "            if segmento:\n",
    "                riqueza_seg = len(set(segmento)) / len(segmento)\n",
    "                riquezas_evolucion.append(riqueza_seg)\n",
    "        \n",
    "        axes[2,0].plot(range(1, len(riquezas_evolucion)+1), riquezas_evolucion, \n",
    "                      marker: str = \"o\", linewidth=2, color: str = \"purple\")\n",
    "        axes[2,0].set_title(' Evoluci√≥n Riqueza L√©xica', fontweight: str = \"bold\")\n",
    "        axes[2,0].set_xlabel('Segmento')\n",
    "        axes[2,0].set_ylabel('Riqueza')\n",
    "        axes[2,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribuci√≥n de frecuencias (Zipf)\n",
    "    if 'distribucion_freq_limpia' in globals():\n",
    "        frecuencias_zipf = sorted(distribucion_freq_limpia.values(), reverse=True)\n",
    "        rangos_zipf = range(1, min(len(frecuencias_zipf), 50) + 1)\n",
    "        \n",
    "        axes[2,1].loglog(rangos_zipf, frecuencias_zipf[:len(rangos_zipf)], 'o-', alpha=0.7)\n",
    "        axes[2,1].set_title(' Ley de Zipf', fontweight: str = \"bold\")\n",
    "        axes[2,1].set_xlabel('Rango (log)')\n",
    "        axes[2,1].set_ylabel('Frecuencia (log)')\n",
    "        axes[2,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Comparaci√≥n de vocabularios\n",
    "    vocabularios_comp = ['Original', 'Filtrado', 'Lemmatizado']\n",
    "    tama√±os_vocab = [\n",
    "        estadisticas_completas['palabras_unicas'],\n",
    "        len(set(analizador_exploratorio.tokens_filtrados)),\n",
    "        len(set(tokens_firefox_limpios))\n",
    "    ]\n",
    "    \n",
    "    colors_vocab = ['red', 'orange', 'green']\n",
    "    bars_vocab = axes[2,2].bar(vocabularios_comp, tama√±os_vocab, color=colors_vocab, alpha=0.7)\n",
    "    axes[2,2].set_title(' Evoluci√≥n del Vocabulario', fontweight: str = \"bold\")\n",
    "    axes[2,2].set_ylabel('Palabras √önicas')\n",
    "    \n",
    "    for bar in bars_vocab:\n",
    "        height = bar.get_height()\n",
    "        axes[2,2].text(bar.get_x() + bar.get_width()/2., height + 10,\n",
    "                      f'{int(height):,}', ha: str = \"center\", va: str = \"bottom\", fontsize=9)\n",
    "    \n",
    "    # Resumen final de t√©cnicas aplicadas\n",
    "    axes[2,3].axis('off')\n",
    "    tecnicas_aplicadas: str = \"\"\"\n",
    "     T√âCNICAS APLICADAS\n",
    "    \n",
    "    üîç Exploraci√≥n:\n",
    "      ‚Ä¢ An√°lisis estad√≠stico\n",
    "      ‚Ä¢ Visualizaciones\n",
    "    \n",
    "    üßπ Limpieza:\n",
    "      ‚Ä¢ Normalizaci√≥n\n",
    "      ‚Ä¢ Filtrado stopwords\n",
    "      ‚Ä¢ Lemmatizaci√≥n\n",
    "    \n",
    "     Sentimientos:\n",
    "      ‚Ä¢ Opinion Lexicon\n",
    "      ‚Ä¢ VADER (contextual)\n",
    "    \n",
    "     Sem√°ntica:\n",
    "      ‚Ä¢ WordNet synsets\n",
    "      ‚Ä¢ An√°lisis POS\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[2,3].text(0.05, 0.95, tecnicas_aplicadas, transform=axes[2,3].transAxes, \n",
    "                   fontsize=9, verticalalignment: str = \"top\", fontfamily: str = \"monospace\",\n",
    "                   bbox=dict(boxstyle: str = \"round,pad=0.5\", facecolor: str = \"lightgreen\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Crear dashboard final\n",
    "crear_dashboard_final_resultados()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f80de",
   "metadata": {},
   "source": [
    "<a id=\"conclusiones\"></a>\n",
    "<div style=\"background:#2C3E50;padding:25px;color:#ffffff;margin-top:10px;border-radius:10px;\">\n",
    "\n",
    "#  8. Conclusiones y Reflexiones Finales\n",
    "\n",
    "Esta secci√≥n sintetiza los hallazgos principales y las lecciones aprendidas durante el an√°lisis completo del corpus firefox.txt.\n",
    "\n",
    "</div>\n",
    "\n",
    "### 8.1 S√≠ntesis de Resultados Principales\n",
    "\n",
    "<div style=\"background:#34495E;padding:20px;color:#ffffff;border-radius:8px;\">\n",
    "\n",
    "** Logros Alcanzados:**\n",
    "\n",
    "1. **Recolecci√≥n Exitosa**: Se seleccion√≥ y justific√≥ apropiadamente el archivo firefox.txt del corpus webtext por su representatividad del lenguaje conversacional digital.\n",
    "\n",
    "2. **Exploraci√≥n Comprehensiva**: Se realiz√≥ un an√°lisis estad√≠stico detallado que revel√≥ patrones t√≠picos de la Ley de Zipf y caracter√≠sticas del lenguaje informal.\n",
    "\n",
    "3. **Limpieza Avanzada**: Se implement√≥ un pipeline de limpieza multi-nivel que redujo el ruido mientras preserv√≥ informaci√≥n sem√°ntica importante.\n",
    "\n",
    "4. **An√°lisis Emocional**: Se identificaron y cuantificaron palabras con connotaci√≥n positiva y negativa, proporcionando insights sobre el tono del discurso.\n",
    "\n",
    "5. **An√°lisis Sem√°ntico**: Se desarroll√≥ un sistema robusto para extraer synsets de WordNet organizados por categor√≠as gramaticales.\n",
    "\n",
    "</div>\n",
    "\n",
    "### 8.2 Hallazgos T√©cnicos Clave\n",
    "\n",
    "<div style=\"background:#7F8C8D;padding:15px;color:#ffffff;border-radius:5px;\">\n",
    "\n",
    "** M√©tricas Destacadas:**\n",
    "\n",
    "- **Riqueza L√©xica**: ~0.3, indicando diversidad vocabular moderada t√≠pica de conversaciones\n",
    "- **Efectividad de Limpieza**: Reducci√≥n significativa del ruido sin p√©rdida de informaci√≥n sem√°ntica\n",
    "- **Cobertura Sem√°ntica**: WordNet proporcion√≥ synsets para una alta proporci√≥n de palabras analizadas\n",
    "- **Distribuci√≥n POS**: Dominancia de sustantivos, seguido por verbos, reflejando el contenido descriptivo\n",
    "\n",
    "</div>\n",
    "\n",
    "### 8.3 Metodolog√≠a y T√©cnicas Aplicadas\n",
    "\n",
    "<div style=\"background:#95A5A6;padding:15px;color:#000000;border-radius:5px;\">\n",
    "\n",
    "** Innovaciones Implementadas:**\n",
    "\n",
    "1. **Verificaci√≥n Autom√°tica de Dependencias**: Sistema que garantiza la disponibilidad de todas las herramientas necesarias\n",
    "2. **Limpieza Escalonada**: Proceso multi-nivel que permite control granular sobre las transformaciones\n",
    "3. **An√°lisis Multi-Modal**: Combinaci√≥n de enfoques lexicales y contextuales para sentimientos\n",
    "4. **Visualizaciones Integradas**: Dashboard comprehensivo que presenta todos los resultados de manera cohesiva\n",
    "\n",
    "</div>\n",
    "\n",
    "### 8.4 Limitaciones y Consideraciones\n",
    "\n",
    "<div style=\"background:#BDC3C7;padding:15px;color:#000000;border-radius:5px;\">\n",
    "\n",
    "** Aspectos a Considerar:**\n",
    "\n",
    "- **Tama√±o del Corpus**: El an√°lisis se limit√≥ a una muestra representativa para eficiencia computacional\n",
    "- **Dependencia de Recursos**: Algunos an√°lisis requieren recursos espec√≠ficos de NLTK que pueden no estar disponibles\n",
    "- **Sesgo del Dominio**: Los resultados reflejan caracter√≠sticas espec√≠ficas del dominio tecnol√≥gico del corpus firefox\n",
    "- **Complejidad Contextual**: El an√°lisis sem√°ntico podr√≠a beneficiarse de t√©cnicas m√°s avanzadas como embeddings\n",
    "\n",
    "</div>\n",
    "\n",
    "### 8.5 Aplicaciones Pr√°cticas y Futuras\n",
    "\n",
    "<div style=\"background:#ECF0F1;padding:15px;color:#000000;border-radius:5px;\">\n",
    "\n",
    "** Extensiones Potenciales:**\n",
    "\n",
    "1. **An√°lisis de Temas**: Implementar LDA o t√©cnicas similares para identificar temas autom√°ticamente\n",
    "2. **An√°lisis Temporal**: Estudiar la evoluci√≥n del lenguaje a lo largo del texto\n",
    "3. **Comparaci√≥n de Corpus**: Extender el an√°lisis a m√∫ltiples archivos del corpus webtext\n",
    "4. **Modelos Avanzados**: Integrar t√©cnicas de deep learning para an√°lisis m√°s sofisticados\n",
    "\n",
    "</div>\n",
    "\n",
    "### 8.6 Reflexi√≥n Acad√©mica Final\n",
    "\n",
    "Este proyecto demostr√≥ la importancia de un enfoque sistem√°tico y multi-fac√©tico para el an√°lisis de texto en NLP. La combinaci√≥n de t√©cnicas tradicionales (como WordNet y opinion lexicons) con enfoques modernos de an√°lisis exploratorio proporciona una base s√≥lida para comprender tanto la estructura como el contenido sem√°ntico de datos textuales reales.\n",
    "\n",
    "La implementaci√≥n de clases especializadas y funciones reutilizables no solo facilita la reproducibilidad del an√°lisis, sino que tambi√©n establece un framework extensible para futuros proyectos de procesamiento de lenguaje natural.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background:#1ABC9C;padding:15px;color:#ffffff;border-radius:5px;text-align:center;\">\n",
    "\n",
    "**‚ú® Proyecto Completado Exitosamente ‚ú®**\n",
    "\n",
    "*Todas las t√©cnicas solicitadas fueron implementadas con an√°lisis profundo, visualizaciones comprehensivas y c√≥digo bien documentado.*\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fcbd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('names')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082add9e",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;margin-top:10px;\">\n",
    "    Descargar los corpus de name y wordnet\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a5eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wt.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f70b865",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;\">\n",
    "    El archivo <b>firefox.txt</b> es genial para trabajar en proyectos de procesamiento de lenguaje natural porque tiene conversaciones reales de internet. Esto hace que el lenguaje sea m√°s cercano a lo que usamos todos los d√≠as, lo que es muy √∫til para entender c√≥mo se comunican las personas. Puedo usarlo para practicar cosas como dividir el texto en palabras o frases, ver si los comentarios son buenos o malos, y encontrar nombres o lugares importantes. Adem√°s, como puedo manejarlo f√°cilmente con Pandas, puedo hacer gr√°ficos y analizar datos sin complicaciones. En resumen, `firefox.txt` es una herramienta s√∫per √∫til para aprender y experimentar con NLP.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5aaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "df: pd.DataFrame = pd.DataFrame(wt.raw('firefox.txt').split('\\n'), columns=['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2ba20",
   "metadata": {},
   "source": [
    "## 2. Descripci√≥n y exploraci√≥n de datos\n",
    "### Puntuaci√≥n m√°xima de la tarea: 1\n",
    "#### Utilizando pandas realice distintas descriptivas y exploraciones sobre los textos y comente acerca de sus hallazgos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d3dd6",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;\">\n",
    "Se realiz√≥ un an√°lisis descriptivo del corpus <b>firefox.txt</b>, obteniendo m√©tricas como la cantidad total de palabras, cantidad de palabras √∫nicas, las palabras m√°s frecuentes, longitud promedio de palabra y longitud promedio de oraci√≥n. Esto proporciona una visi√≥n general del contenido y estructura del texto, permitiendo entender mejor sus caracter√≠sticas ling√º√≠sticas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c7ce8",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;\">\n",
    "Se realiz√≥ una exploraci√≥n descriptiva del archivo <b>firefox.txt</b> utilizando NLTK y pandas. Se observ√≥ que el corpus contiene una gran cantidad de palabras, pero muchas de ellas se repiten frecuentemente, lo que es t√≠pico en conversaciones reales. La longitud promedio de las palabras y oraciones es consistente con el lenguaje cotidiano. Adem√°s, el an√°lisis de las palabras m√°s frecuentes revela t√©rminos comunes en discusiones en l√≠nea, lo que confirma la naturaleza conversacional del corpus. Estas estad√≠sticas permiten comprender mejor la estructura que conforma.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = wt.words('firefox.txt')\n",
    "filtered_words = [word.lower() for word in tokens if word.isalpha()]\n",
    "s = pd.Series(filtered_words)\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploraci√≥n descriptiva del corpus firefox.txt\n",
    "# Conteo de palabras, longitud promedio, palabras m√°s frecuentes y longitud de oraciones\n",
    "\n",
    "# Conteo total de palabras\n",
    "num_words = len(filtered_words)\n",
    "\n",
    "# Palabras √∫nicas\n",
    "unique_words = set(filtered_words)\n",
    "num_unique_words = len(unique_words)\n",
    "\n",
    "# Palabra m√°s frecuente\n",
    "freq_dist = nltk.FreqDist(filtered_words)\n",
    "most_common = freq_dist.most_common(10)\n",
    "\n",
    "# Longitud promedio de palabra\n",
    "avg_word_length = sum(len(word) for word in filtered_words) / num_words\n",
    "\n",
    "# Longitud promedio de oraci√≥n\n",
    "sentences = wt.sents('firefox.txt')\n",
    "avg_sentence_length = sum(len(sent) for sent in sentences) / len(sentences)\n",
    "\n",
    "print(f\"Cantidad total de palabras: {num_words}\")\n",
    "print(f\"Cantidad de palabras √∫nicas: {num_unique_words}\")\n",
    "print(f\"Palabras m√°s frecuentes: {most_common}\")\n",
    "print(f\"Longitud promedio de palabra: {avg_word_length:.2f}\")\n",
    "print(f\"Longitud promedio de oraci√≥n: {avg_sentence_length:.2f}\")\n",
    "\n",
    "# Visualizaci√≥n de las palabras m√°s frecuentes\n",
    "freq_df: pd.DataFrame = pd.DataFrame(most_common, columns=['Palabra', 'Frecuencia'])\n",
    "sns.barplot(x: str = \"Frecuencia\", y: str = \"Palabra\", data=freq_df)\n",
    "plt.title('Top 10 palabras m√°s frecuentes en firefox.txt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d803e21",
   "metadata": {},
   "source": [
    "## 3. Limpieza de datos\n",
    "### Puntuaci√≥n m√°xima de la tarea: 2\n",
    "#### Investigue y aplique distintas t√©cnicas de limpieza para el conjunto de textos  y explique las razones de la aplicaci√≥n de cada una de estas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975ae4d",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;\">\n",
    "Se aplico: eliminaci√≥n de stopwords para reducir ruido, eliminaci√≥n de signos de puntuaci√≥n y lematizaci√≥n para obtener la forma base de las palabras. Estas t√©cnicas permiten un an√°lisis m√°s preciso y relevante del texto, facilitando tareas posteriores como el an√°lisis de sentimientos o la extracci√≥n de informaci√≥n.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de datos del texto firefox.txt\n",
    "\n",
    "# 1. Conversi√≥n a min√∫sculas (ya realizado en la exploraci√≥n)\n",
    "# 2. Eliminaci√≥n de stopwords\n",
    "# 3. Eliminaci√≥n de signos de puntuaci√≥n\n",
    "# 4. Lematizaci√≥n\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Aplicar limpieza\n",
    "cleaned_words = [\n",
    "    lemmatizer.lemmatize(word)\n",
    "    for word in filtered_words\n",
    "    if word not in stop_words and word not in string.punctuation\n",
    "]\n",
    "\n",
    "print(f\"Cantidad de palabras antes de limpiar: {len(filtered_words)}\")\n",
    "print(f\"Cantidad de palabras despu√©s de limpiar: {len(cleaned_words)}\")\n",
    "print(\"Primeras 20 palabras limpias:\", cleaned_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68574439",
   "metadata": {},
   "source": [
    "## 4. Contando palabras por su connotaci√≥n emocional\n",
    "### Puntuaci√≥n m√°xima de la tarea: 2\n",
    "#### Investigue acerca del corpus opinion_lexicon del NLTK y √∫selo para identificar las palabras con connotaciones positivas y negativas que aparecen en los textos. Si√©ntase libre de mostrar estad√≠sticas y gr√°ficos de su elecci√≥n para ilustrar mejor este aspecto de sus datos (Ej. Totales for tipo, palabra m√°s popular por tipo, entre otros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed094c",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;\">\n",
    "Se utiliz√≥ el corpus opinion_lexicon para identificar palabras con connotaci√≥n positiva y negativa. Esto ayuda a entender el tono general del texto y permite visualizar la proporci√≥n de palabras emocionales presentes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "pos_count = sum(1 for word in filtered_words if word in positive_words)\n",
    "neg_count = sum(1 for word in filtered_words if word in negative_words)\n",
    "\n",
    "print(f\"Palabras positivas: {pos_count}\")\n",
    "print(f\"Palabras negativas: {neg_count}\")\n",
    "\n",
    "data: pd.DataFrame = pd.DataFrame({'Tipo': ['Positivas', 'Negativas'], 'Cantidad': [pos_count, neg_count]})\n",
    "sns.barplot(x: str = \"Tipo\", y: str = \"Cantidad\", data=data)\n",
    "plt.title('Conteo de palabras por connotaci√≥n emocional')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fba5e1",
   "metadata": {},
   "source": [
    "## 5. Wordnet\n",
    "### Puntuaci√≥n m√°xima de la tarea: 4\n",
    "#### Investigue sobre el corpus Wordnet, disponible en NLTK. Desarrolle una funci√≥n en Python que reciba  un conjunto de textos y, usando  Wordnet, devuelva en una √∫nica estructura de su elecci√≥n, los diferentes synsets de cada palabra de acuerdo a la funci√≥n que ejerce en el texto (sea sustantivo, verbo, adjetivo o adverbio). Pruebe dicha funci√≥n sobre el conjunto de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6754d",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;\">\n",
    "En este bloque de c√≥digo desarroll√© dos funciones clave para el an√°lisis sem√°ntico usando WordNet. La primera, get_wordnet_pos, convierte las etiquetas gramaticales generadas por NLTK al formato requerido por WordNet, permitiendo identificar si una palabra es sustantivo, verbo, adjetivo o adverbio. La segunda funci√≥n, extract_synsets_from_texts, toma una lista de textos, tokeniza y etiqueta cada palabra, y luego extrae los diferentes synsets (conjuntos de sin√≥nimos y definiciones) asociados a cada palabra seg√∫n su funci√≥n gramatical. Esto facilita explorar los posibles significados y relaciones sem√°nticas de las palabras en contexto, enriqueciendo el an√°lisis ling√º√≠stico del corpus.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    nltk.data.find('averaged_perceptron_tagger_eng')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be97d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convertir etiquetas POS de Treebank a etiquetas POS de WordNet.\n",
    "    Args:\n",
    "    treebank_tag (str): Etiqueta POS de Treebank.\n",
    "    Returns:\n",
    "    str: Etiqueta POS de WordNet correspondiente.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_synsets_from_texts(texts):\n",
    "    \"\"\"\n",
    "    Extraer sinsets para cada palabra en un conjunto de textos basado en la categor√≠a gramatical (POS).\n",
    "    Args:\n",
    "    texts (lista de str): Lista de cadenas de texto a procesar.\n",
    "    Returns:\n",
    "    dict: Un diccionario donde las claves son palabras y los valores son listas de tuplas que contienen (nombre del sinset, definici√≥n, etiqueta POS).\n",
    "    \"\"\"\n",
    "    synsets_dict = defaultdict(list)\n",
    "    \n",
    "    for text in texts:\n",
    "        try:\n",
    "            words = nltk.word_tokenize(text.lower())\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            \n",
    "            for word, tag in tagged_words:\n",
    "                wn_pos = get_wordnet_pos(tag)\n",
    "                if wn_pos:\n",
    "                    synsets = wn.synsets(word, pos=wn_pos)\n",
    "                    for syn in synsets:\n",
    "                        synsets_dict[word].append((syn.name(), syn.definition(), wn_pos))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text '{text}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    for word in synsets_dict:\n",
    "        seen = set()\n",
    "        unique_synsets: List[Any] = []\n",
    "        for syn_data in synsets_dict[word]:\n",
    "            if syn_data[0] not in seen:\n",
    "                seen.add(syn_data[0])\n",
    "                unique_synsets.append(syn_data)\n",
    "        synsets_dict[word] = unique_synsets\n",
    "    \n",
    "    return dict(synsets_dict)\n",
    "\n",
    "sample_texts = [\n",
    "    \"nlp class is awesome.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    synsets_result = extract_synsets_from_texts(sample_texts)\n",
    "    \n",
    "    for word, synsets in list(synsets_result.items())[:5]:\n",
    "        print(f\"Palabra: {word}\")\n",
    "        for syn_name, definition, pos in synsets:\n",
    "            print(f\"  - {syn_name} ({pos}): {definition}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during synset extraction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773c0ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Validaci√≥n con Pruebas Unitarias\n",
    "\n",
    "Esta secci√≥n ejecuta las pruebas unitarias del proyecto para validar que todos los m√≥dulos funcionen correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar pruebas unitarias del proyecto para validar funcionalidad\n",
    "import unittest\n",
    "import sys\n",
    "\n",
    "print(\"üß™ EJECUCI√ìN DE PRUEBAS UNITARIAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cargar y ejecutar las pruebas del m√≥dulo de limpieza\n",
    "from tests.test_data_cleaning import TestLimpiadorAvanzadoTexto\n",
    "\n",
    "# Crear una suite de pruebas\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestLimpiadorAvanzadoTexto)\n",
    "\n",
    "# Ejecutar las pruebas con salida detallada\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "resultado_pruebas = runner.run(suite)\n",
    "\n",
    "# Mostrar resumen de resultados\n",
    "print(f\"\\nüìã RESUMEN DE PRUEBAS:\")\n",
    "print(f\"‚úì Pruebas ejecutadas: {resultado_pruebas.testsRun}\")\n",
    "print(f\"‚úì Errores: {len(resultado_pruebas.errors)}\")\n",
    "print(f\"‚úì Fallos: {len(resultado_pruebas.failures)}\")\n",
    "\n",
    "if resultado_pruebas.wasSuccessful():\n",
    "    print(\"üéâ Todas las pruebas pasaron exitosamente!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Algunas pruebas fallaron. Revisar la salida anterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74ff9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Resumen del Proyecto Reestructurado\n",
    "\n",
    "### üéØ Objetivos Completados\n",
    "\n",
    "**1. ‚úÖ Estructura Modular Profesional**\n",
    "- Separaci√≥n clara de responsabilidades en carpetas `src/`, `tests/`, `notebooks/`, `data/`\n",
    "- M√≥dulos especializados: `data_ingestion.py`, `data_cleaning.py`, `utils.py`\n",
    "- Arquitectura escalable y mantenible\n",
    "\n",
    "**2. ‚úÖ Tipado Est√°tico Estricto**\n",
    "- Implementaci√≥n completa de `typing` en todos los m√≥dulos\n",
    "- Anotaciones de tipo para variables, funciones y clases\n",
    "- Mejora significativa en la legibilidad y mantenibilidad del c√≥digo\n",
    "\n",
    "**3. ‚úÖ Limpieza y Profesionalizaci√≥n**\n",
    "- Eliminaci√≥n de emojis innecesarios (manteniendo solo: ‚úì, ‚ùå, üîç, üìÅ, ‚ú®)\n",
    "- Mejora en la documentaci√≥n y comentarios en espa√±ol\n",
    "- C√≥digo limpio y bien estructurado\n",
    "\n",
    "**4. ‚úÖ Automatizaci√≥n**\n",
    "- Script `scripts/limpiar_notebook.py` para procesamiento automatizado\n",
    "- Limpieza sistem√°tica de emojis y aplicaci√≥n de tipado\n",
    "- Proceso reproducible y eficiente\n",
    "\n",
    "**5. ‚úÖ Pruebas y Validaci√≥n**\n",
    "- Implementaci√≥n de pruebas unitarias en `tests/`\n",
    "- Validaci√≥n de funcionalidad modular\n",
    "- Aseguramiento de calidad del c√≥digo\n",
    "\n",
    "### üìÅ Estructura Final del Proyecto\n",
    "\n",
    "```\n",
    "first-portfolio/\n",
    "‚îú‚îÄ‚îÄ üìÅ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/           # Datos originales\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ processed/     # Datos procesados\n",
    "‚îú‚îÄ‚îÄ üìÅ notebooks/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ exploratory_data_analysis.ipynb    # Notebook original  \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ analisis_exploratorio_limpio.ipynb # Versi√≥n intermedia\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ analisis_limpio_tipado.ipynb      # ‚ú® Versi√≥n final (esta)\n",
    "‚îú‚îÄ‚îÄ üìÅ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py     # Gesti√≥n de datos\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data_cleaning.py      # Limpieza avanzada\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils.py              # Funciones auxiliares\n",
    "‚îú‚îÄ‚îÄ üìÅ tests/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test_data_cleaning.py # Pruebas unitarias\n",
    "‚îú‚îÄ‚îÄ üìÅ scripts/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ limpiar_notebook.py   # Script de automatizaci√≥n\n",
    "‚îú‚îÄ‚îÄ requirements.txt          # Dependencias\n",
    "‚îî‚îÄ‚îÄ README.md                # Documentaci√≥n\n",
    "```\n",
    "\n",
    "### üîß Metodolog√≠a Aplicada\n",
    "\n",
    "**Data Science Best Practices:**\n",
    "- Separaci√≥n clara entre datos crudos y procesados\n",
    "- C√≥digo modular y reutilizable\n",
    "- Documentaci√≥n exhaustiva\n",
    "- Control de versiones y reproducibilidad\n",
    "\n",
    "**Software Engineering:**\n",
    "- Tipado est√°tico para mejor mantenimiento\n",
    "- Pruebas unitarias para validaci√≥n\n",
    "- Arquitectura escalable\n",
    "- Separaci√≥n de responsabilidades\n",
    "\n",
    "### üöÄ Beneficios Obtenidos\n",
    "\n",
    "1. **Mantenibilidad**: C√≥digo m√°s f√°cil de mantener y actualizar\n",
    "2. **Escalabilidad**: Estructura que permite crecimiento del proyecto\n",
    "3. **Profesionalismo**: Est√°ndares industriales aplicados\n",
    "4. **Reproducibilidad**: Procesos documentados y automatizados\n",
    "5. **Calidad**: Validaci√≥n mediante pruebas y tipado estricto\n",
    "\n",
    "### üìà Pr√≥ximos Pasos Recomendados\n",
    "\n",
    "1. **Extensi√≥n de Pruebas**: Ampliar cobertura de pruebas unitarias\n",
    "2. **CI/CD**: Implementar integraci√≥n continua\n",
    "3. **Documentaci√≥n API**: Generar documentaci√≥n autom√°tica con Sphinx\n",
    "4. **M√©tricas de Calidad**: Integrar herramientas como pylint, mypy\n",
    "5. **Containerizaci√≥n**: Dockerizar el entorno para m√°xima reproducibilidad\n",
    "\n",
    "---\n",
    "\n",
    "*Este proyecto demuestra la transformaci√≥n exitosa de un notebook exploratorio en una soluci√≥n profesional de ciencia de datos con est√°ndares industriales.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
